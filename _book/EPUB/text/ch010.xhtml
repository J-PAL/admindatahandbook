<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch010.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
</head>
<body epub:type="bodymatter">
<section id="discavoid" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Balancing Privacy and Data Usability: An Overview of Disclosure Avoidance Methods</h1>
<p><em>Ian M. Schmutte (University of Georgia)</em><br />
<em>Lars Vilhuber (Cornell University)</em></p>
<p>The purpose of this Handbook is to provide guidance on how to enable broader but ethical and legal access to data. Within the Five Safes framework <span class="citation" data-cites="desai2016">(Desai, Ritchie, and Welpton <a href="ch024.xhtml#ref-desai2016" role="doc-biblioref">2016</a>)</span>, data providers need to create <em>safe data</em> that can be provided to trusted <em>safe people</em> for use within <em>safe settings</em> (chapter <a href="#security">2</a>), subject to legal and contractual safeguards (chapter <a href="#dua">3</a>). Related, but distinct, is the question of how to create <em>safe outputs</em> from researchers’ findings before those findings finally make their way into the public through, for example, policy briefs or the academic literature. The processes used to create safe data and safe outputs (manipulations that render data less sensitive and therefore more appropriate for public release) are generally referred to as statistical disclosure limitation (SDL).<a href="#fn1" class="footnote-ref" id="fnref1" epub:type="noteref">1</a> This chapter will describe techniques traditionally used within the field of SDL, pointing at methods as well as metrics to assess the resultant statistical quality and sensitivity of the data. Newer approaches, generally referred to as <em>formal privacy methods</em>, are described in chapter <a href="#diffpriv">6</a>.</p>
<p>At their core, SDL methods prevent outsiders from learning too much about any one record in the data <span class="citation" data-cites="dalenius_towards_1977">(Dalenius <a href="ch024.xhtml#ref-dalenius_towards_1977" role="doc-biblioref">1977</a>)</span> by deliberately and judiciously adding distortions. Ideally, these distortions maintain the validity of the data for statistical analysis but strongly reduce the ability to isolate records and infer precise information about individual people, firms, or cases. In general, it is necessary to sacrifice validity in order to prevent disclosure <span class="citation" data-cites="goroff_balancing_2015 abowd_economic_2015">(Goroff <a href="ch024.xhtml#ref-goroff_balancing_2015" role="doc-biblioref">2015</a>; Abowd and Schmutte <a href="ch024.xhtml#ref-abowd_economic_2015" role="doc-biblioref">2015</a>)</span>. It is therefore important for data custodians to bear this trade-off in mind when deciding whether and how to use SDL.</p>
<p>One key challenge for implementing privacy systems lies in choosing the amount or type of privacy to provide. Answering this question requires some way to understand the individual and social value of privacy. <span class="citation" data-cites="abowd_economic_2019">J. M. Abowd and Schmutte (<a href="ch024.xhtml#ref-abowd_economic_2019" role="doc-biblioref">2019</a>)</span> discuss the question of optimal privacy protection <span class="citation" data-cites="hsu_differential_2014">(see also Hsu et al. <a href="ch024.xhtml#ref-hsu_differential_2014" role="doc-biblioref">2014</a> in the specific context of differential privacy)</span>. For an illustration, see <span class="citation" data-cites="spencer_effects_2015">Spencer and Seeskin (<a href="ch024.xhtml#ref-spencer_effects_2015" role="doc-biblioref">2015</a>)</span>, who use a calibration exercise to study the costs (measured in misallocated congressional seats) of reduced accuracy in population census data.</p>
<p>art of the social value of privacy arises from its relationship to scientific integrity. While the law of information recovery suggests that improved privacy must come at the cost of increased error in published statistics, these effects might be mitigated through two distinct channels. First, people may be more truthful in surveys if they believe their data are not at risk <span class="citation" data-cites="couper_risk_2008">(Couper et al. <a href="ch024.xhtml#ref-couper_risk_2008" role="doc-biblioref">2008</a>)</span>. Second, work in computer science and statistics <span class="citation" data-cites="dwork_generalization_2015 dwork_fienberg_2018 cummings_adaptive_2016">(Dwork et al. <a href="ch024.xhtml#ref-dwork_generalization_2015" role="doc-biblioref">2015</a>; Dwork and Ullman <a href="ch024.xhtml#ref-dwork_fienberg_2018" role="doc-biblioref">2018</a>; Cummings et al. <a href="ch024.xhtml#ref-cummings_adaptive_2016" role="doc-biblioref">2016</a>)</span> suggests a somewhat surprising benefit of differential privacy: protection against overfitting.</p>
<p>There are three factors that a data custodian should bear in mind when deciding whether and how to implement an SDL system in support of making data accessible. First, it is necessary to clarify the specific privacy requirements based on the nature of the underlying data, institutional and policy criteria, and ethical considerations. In addition, the custodian, perhaps in consultation with users, should clarify what sorts of analyses the data will support. Finally, SDL is often part of a broader system to protect sensitive data that can also involve access restrictions and other technical barriers. The broader system may allow for less stringent SDL techniques when providing data to researchers in secure environments than would be possible if data were to be released as unrestricted public use data.^[[Chapter <a href="#iab">7</a> on the RDC-IAB provides a good illustration of how various SDL methods are combined with different access methods to provide multiple combinations of analytic validity and risk of disclosure.] This implies that the chapter will not provide a recommendation for a “best” method, since no such globally optimal method exists in isolation.</p>
<p>Rather, this chapter provides an overview of the concepts and more widely used methods of SDL. Relative to other primers that cover similar material, this text focuses more closely on the advantages and disadvantages of various methods from the perspective of data users. This chapter can serve as a reference that data providers and data users can employ to discuss which forms of SDL are appropriate and will satisfy the needs of both parties. In particular, there is a focus on how common SDL tools affect different types of statistical analysis as well as the kind of confidentiality protections these tools support, drawing heavily on <span class="citation" data-cites="abowd_economic_2015">Abowd and Schmutte (<a href="ch024.xhtml#ref-abowd_economic_2015" role="doc-biblioref">2015</a>)</span>. SDL is a broad topic with a vast literature, starting with <span class="citation" data-cites="fellegi_question_1972">Fellegi (<a href="ch024.xhtml#ref-fellegi_question_1972" role="doc-biblioref">1972</a>)</span>. Naturally, this brief summary is not a replacement for the textbook treatment of SDL in <span class="citation" data-cites="duncan2011">Duncan, Elliot, and Salazar-González (<a href="ch024.xhtml#ref-duncan2011" role="doc-biblioref">2011</a>)</span>. Finally, SDL methods must be implemented and deployed, and the chapter provides pointers to existing off-the-rack tools in a variety of platforms (Python, R, and Stata). Readers might also consult other summaries and guides, such as <span class="citation" data-cites="dupriez2010">Dupriez and Boyko (<a href="ch024.xhtml#ref-dupriez2010" role="doc-biblioref">2010</a>)</span>, <span class="citation" data-cites="world_bank_dime_nodate">World Bank (<a href="ch024.xhtml#ref-world_bank_dime_nodate" role="doc-biblioref">n.d.</a>)</span>, <span class="citation" data-cites="kopper_j-pal_2020">Kopper, Sautmann, and Turitto (<a href="ch024.xhtml#ref-kopper_j-pal_2020" role="doc-biblioref">2020</a>)</span>, and <span class="citation" data-cites="liu2020">Liu (<a href="ch024.xhtml#ref-liu2020" role="doc-biblioref">2020</a>)</span>.</p>
<section id="purpose-of-statistical-disclosure-limitation-methods-definitions-and-context" class="level2" data-number="5.1">
<h2 data-number="5.1"><span class="header-section-number">5.1</span> Purpose of Statistical Disclosure Limitation Methods: Definitions and Context</h2>
<!-- (1-1.5 pages) -->
<p>A clear and precise sense of what constitutes an unauthorized disclosure is a prerequisite to implementing SDL. Are all data items equally sensitive? How much more should one be able to learn about certain classes of people, firms, villages, etc.? Note that even when trusted researchers (<em>safe people</em>) can be sworn to secrecy, the ultimate goal is to publish using information gleaned from the data, and the final audience can never be considered trusted.<a href="#fn2" class="footnote-ref" id="fnref2" epub:type="noteref">2</a></p>
<p>The key concepts are privacy and confidentiality. Privacy can be viewed, in this context, as the right to restrict others’ access to personal information, whether through query or through observation <span class="citation" data-cites="hirshleifer_privacy_1980">(Hirshleifer <a href="ch024.xhtml#ref-hirshleifer_privacy_1980" role="doc-biblioref">1980</a>)</span>. Confidentiality pertains to data that have already been collected and describes the principle that the data should not be used in ways that could harm the persons that provided their information.</p>
<div class="bboxfix">
<p>
For example, Ann, who is asked to participate in a study about health behaviors, has a <em>privacy</em> right to refuse to answer a question about smoking. If she does answer the question, it would breach <em>confidentiality</em> if her response was then used by an insurance company to adjust her premiums <span class="citation"><span class="citation" data-cites="duncan_private_1993">(Duncan, Jabine, and Wolf <a href="ch024.xhtml#ref-duncan_private_1993" role="doc-biblioref">1993</a>)</span></span>.
</p>
</div>
<p><span class="citation" data-cites="harris-kojetin_statistical_2005">Harris-Kojetin et al. (<a href="ch024.xhtml#ref-harris-kojetin_statistical_2005" role="doc-biblioref">2005</a>)</span> define disclosure as the “inappropriate attribution of information to a data subject, whether an individual or an organization” <span class="citation" data-cites="harris-kojetin_statistical_2005">(Harris-Kojetin et al. <a href="ch024.xhtml#ref-harris-kojetin_statistical_2005" role="doc-biblioref">2005</a>, 4)</span>. They proceed to describe three different types of disclosure. An <em>identity disclosure</em> is one where it is possible to learn that a particular record or data item belongs to a particular participant (individual or organization). An <em>attribute disclosure</em> happens if publication of the data reveals an attribute of a participant. Note that an <em>identity disclosure</em> necessarily entails <em>attribute disclosure</em>, but the reverse is not the case.</p>
<div class="bboxfix">
<p>
In the hypothetical health study, if Ann responds that she is a smoker, an <em>identity disclosure</em> would mean someone can determine which record is hers and therefore can also learn that she is a smoker—an <em>attribute disclosure</em>. However, an attribute disclosure could also occur if someone knows that Ann was in the study, they know that Ann lives in a particular zip code, and the data reveal that all participants from that zip code are also smokers. Her full record was not revealed, but confidentiality was breached all the same.
</p>
</div>
<p>With these concepts in mind, it is necessary to ask whether it is sufficient to prevent blatant all-or-nothing identity or attribute disclosures: usually not, as it may be possible to learn a sensitive attribute with high, but not total, certainty. This is called an <em>inferential disclosure</em> <span class="citation" data-cites="dalenius_towards_1977 duncan_disclosure-limited_1986">(Dalenius <a href="ch024.xhtml#ref-dalenius_towards_1977" role="doc-biblioref">1977</a>; Duncan and Lambert <a href="ch024.xhtml#ref-duncan_disclosure-limited_1986" role="doc-biblioref">1986</a>)</span>.</p>
<div class="bboxfix">
<p>
Suppose Ann’s health insurer knows that Ann is in the data and that she lives in a particular zip code. If the data have 100 records from that zip code and 99 are smokers, then the insurer has learned Ann’s smoking status with imperfect but high precision.
</p>
</div>
<p>In addition to deciding what kinds of disclosure can be tolerated and to what extent, in many cases it may also be meaningful to decide which characteristics are and are not sensitive. Smoking behavior may nowadays be regarded as sensitive, but depending on the context, gender might not be. In the case of business data, total sales volume or total payroll are highly sensitive trade secrets.</p>
<p>Generally, the county in which the business is located or the industry in which the business operates might not be sensitive, but consider a survey of self-employed business people: the location of the business might be the home address, which might be considered highly sensitive. These decisions on what is sensitive affect the implementation of a privacy protection system.<a href="#fn3" class="footnote-ref" id="fnref3" epub:type="noteref">3</a></p>
<p>However, additional care must be taken because variables that are not inherently sensitive can still be used to isolate and identify records. Such variables are sometimes referred to as <em>quasi-identifiers</em> and they can be exploited for <em>re-identification</em> attacks. In business data, if the data show that there is only one firm operating in a particular county and sector, then their presence inherently leads to identity disclosure. Many of the traditional approaches to SDL operate in large part by attempting to prevent re-identification.<a href="#fn4" class="footnote-ref" id="fnref4" epub:type="noteref">4</a> <span class="citation" data-cites="garfinkel_-identification_2015">Garfinkel (<a href="ch024.xhtml#ref-garfinkel_-identification_2015" role="doc-biblioref">2015</a>)</span> discusses techniques for de-identifying data and the many ways in which modern computing tools and a data-rich environment may render effective de-identification impossible, reinforcing the growing need for formal privacy models like differential privacy.</p>
<!-- > Confidential should mean that the dissemination of data in a manner that would allow public identification of the respondent or would in any way be harmful to him is prohibited and that the data are immune from legal process. (SOURCE?) -->
<p>SDL methods may be required for legal and ethical reasons. Institutional review boards (IRBs) require that individual’s well-being be protected (see chapter <a href="#irb">4</a> on IRBs). Legal mandates may intersect with ethical concerns, or prescribe certain (minimal) criteria. Thus, the US Health Insurance Portability and Accountability Act of 1996 (HIPAA) <span class="citation" data-cites="us_department_of_health__human_services_health_nodate">(U.S. Department of Health &amp; Human Services <a href="ch024.xhtml#ref-us_department_of_health__human_services_health_nodate" role="doc-biblioref">n.d.</a>)</span> has precise definitions of variables that need to be removed in order to comply with the law’s mandate of de-identification <span class="citation" data-cites="department_of_health_and_human_services_methods_2012">(Department of Health and Human Services <a href="ch024.xhtml#ref-department_of_health_and_human_services_methods_2012" role="doc-biblioref">2012</a>)</span>. The European Union General Data Protection Regulation (GDPR) came into effect in 2018 and has defined both the way researchers can access data and the requirements for disclosure limitation <span class="citation" data-cites="cohen_towards_2020 greene_adjusting_2019 molnar-gabor_germany_2018">(Cohen and Nissim <a href="ch024.xhtml#ref-cohen_towards_2020" role="doc-biblioref">2020</a>; Greene et al. <a href="ch024.xhtml#ref-greene_adjusting_2019" role="doc-biblioref">2019</a>; Molnár-Gábor <a href="ch024.xhtml#ref-molnar-gabor_germany_2018" role="doc-biblioref">2018</a>)</span>. Similar laws are emerging around the world and will define both minimal requirements and limits of SDL and other access controls. The California Consumer Privacy Act (CCPA) <span class="citation" data-cites="marini_comparing_2018">(Marini, Kateifides, and Bates <a href="ch024.xhtml#ref-marini_comparing_2018" role="doc-biblioref">2018</a>)</span> and the Brazilian Lei Geral de Proteção de Dados (LGPD) <span class="citation" data-cites="black_6_2020">(Black, Ramos, and Biscardi <a href="ch024.xhtml#ref-black_6_2020" role="doc-biblioref">2020</a>)</span> came into effect in 2020, and India is currently considering such a law <span class="citation" data-cites="panakal_indias_2019">(Panakal <a href="ch024.xhtml#ref-panakal_indias_2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>Finally, note that there is a parallel concept of non-statistical disclosure limitation that is a complementary part of secure data dissemination. This applies to the metadata—like codebooks, data descriptions, and other summary information—that can leak potentially sensitive information. For example, data documentation might reveal that only certain geographic areas were included in a particular collection, information that could be used as an element in a re-identification attack. While typically not considered quantitative disclosure avoidance, some of the same concepts described here can apply to such metadata as well. For instance, removing mention of the collection area from the documentation is akin to <a href="ch010.xhtml#suppression">suppression</a>, while only revealing broad regions of data collection is akin to <a href="ch010.xhtml#coarsening">coarsening</a>.</p>
</section>
<section id="methods" class="level2" data-number="5.2">
<h2 data-number="5.2"><span class="header-section-number">5.2</span> Methods</h2>
<p>There are many different SDL methods, and the decision of which to use depends on what needs to be protected, how their use will affect approved analyses, and their technical properties. At a high level, think of an SDL system as a mechanism that takes the raw confidential data, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>, as inputs and produces a modified data set, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>D</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{D}</annotation></semantics></math>. The researcher then conducts their analysis with the modified <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>D</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{D}</annotation></semantics></math>. Ideally, the researcher can do their analysis as planned, but the risk of disclosure in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>D</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{D}</annotation></semantics></math> is reduced.</p>
<p>Researchers generally need to consider all of the design features that went into producing the data used for an analysis. Most already do so in the context of surveys where design measures are incorporated into the analysis—often directly in software packages. Some of these adjustments may already take into account various SDL techniques. Traditional survey design adjustments can consider <a href="ch010.xhtml#sampling">sampling</a>. Some forms of <a href="ch010.xhtml#coarsening">coarsening</a> may already be amenable to adjustment using various clustering techniques, such as <span class="citation" data-cites="moulton_random_1986 cameron_practitioners_2015">Moulton (<a href="ch024.xhtml#ref-moulton_random_1986" role="doc-biblioref">1986</a>; see Cameron and Miller <a href="ch024.xhtml#ref-cameron_practitioners_2015" role="doc-biblioref">2015</a>)</span>.</p>
<p>More generally, the inclusion of edits to the data done in service of disclosure limitation is less well supported by, and less well integrated in, standard research methods. <span class="citation" data-cites="abowd_economic_2015">Abowd and Schmutte (<a href="ch024.xhtml#ref-abowd_economic_2015" role="doc-biblioref">2015</a>)</span> argue that the analyses of SDL-laden data are inherently compromised because the details of the SDL protections cannot be disclosed. If the details cannot be disclosed, the consequences for inference are unknowable and, as they show, may be substantial. Regression models, regression discontinuity designs, and instrumental variables models are generally affected when SDL is present. The exact nature of any bias or inconsistency will depend on whether SDL was applied to explanatory variables, dependent variables, instruments, or all of the above. Furthermore, it is not always the case that SDL induces an attenuating bias.</p>
<p>With these goals in mind, following <span class="citation" data-cites="abowd_economic_2015">Abowd and Schmutte (<a href="ch024.xhtml#ref-abowd_economic_2015" role="doc-biblioref">2015</a>)</span>, this chapter distinguishes between <em>ignorable</em> and <em>non-ignorable</em> SDL systems. Briefly, SDL is <em>ignorable</em> for a particular analysis if the analysis can be performed on the modified data, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>D</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{D}</annotation></semantics></math>, as though it were the true data. In a non-ignorable analysis, the result differs in some material way when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>D</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{D}</annotation></semantics></math> is substituted for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>. When the SDL method is <em>known</em>, then it may be possible for the researcher to perform an <em>SDL-aware</em> analysis that corrects for non-ignorability. However, SDL methods are generally not ignorable except in certain specific applications.</p>
<p>The chapter briefly outlines several of the methods most commonly used within national statistical offices. For interested readers, <span class="citation" data-cites="harris-kojetin_statistical_2005">Harris-Kojetin et al. (<a href="ch024.xhtml#ref-harris-kojetin_statistical_2005" role="doc-biblioref">2005</a>)</span><a href="#fn5" class="footnote-ref" id="fnref5" epub:type="noteref">5</a> describe how SDL systems are implemented in the US statistical system, while <span class="citation" data-cites="dupriez2010">Dupriez and Boyko (<a href="ch024.xhtml#ref-dupriez2010" role="doc-biblioref">2010</a>)</span> offers a more multinational perspective.</p>
<section id="de-identification" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1"><span class="header-section-number">5.2.1</span> De-Identification</h3>
<p>In general, it is good practice to remove any variables from the data that are not needed for data processing or analysis and that could be considered direct identifiers. This is often referred to as de-identification. What constitutes “direct identifiers” may differ on the context, but generally comprises any variable that might directly link to confidential information: names, account or identifier numbers, and sometimes exact birth dates or exact geo-identifiers.<a href="#fn6" class="footnote-ref" id="fnref6" epub:type="noteref">6</a> HIPAA defines sixteen identifiers that must be removed in order to comply with the law. It may be necessary to preserve identifiers through parts of the data processing or analysis if they are key variables needed for record linking. In field experiments, the identities of treatment and control units may need to be merged with an administrative data set. It is also sometimes necessary to use direct identifiers to link records between surveys and administrative data, or precise geographic coordinates may be needed to compute distances as part of the analysis. If possible, the data provider should facilitate record linking while the data are secure and before they are shared with the research team.</p>
</section>
<section id="suppression" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2"><span class="header-section-number">5.2.2</span> Suppression</h3>
<!-- Also mention rules that point to "suppress regression coefficients when derived from less than x% of sample, or the p-percent rule" as they are applied to "safe output" rules in RDC and MOU scenarios. -->
<p>Suppression is perhaps the most common form of SDL and one of the oldest <span class="citation" data-cites="fellegi_question_1972">(Fellegi <a href="ch024.xhtml#ref-fellegi_question_1972" role="doc-biblioref">1972</a>)</span>. In their most basic form, suppression rules work as follows:</p>
<ol type="1">
<li>Model the sensitivity of a particular data item, table cell, or observation (disclosure risk).</li>
<li>Do not allow the release of data items that have excessive disclosure risk (primary suppression).</li>
<li>Do not allow the release of other data from which the sensitive item can be calculated (complementary suppression).</li>
</ol>
<p>Suppression rules can be applied to microdata: the sensitive observations are removed from the microdata, or to tabular data, where the relevant cells are suppressed.</p>
<p>In the case of business microdata, a firm that is unique in its county and industry might be flagged as having high disclosure risk and eliminated from the data. Another less damaging possibility is that just the sensitive attributes are suppressed, so a researcher would still know that there was a firm operating in that industry and location but not the other attributes. For tabular data, the principle is the same. Continuing with the business application, suppose there is one large firm and several smaller competitors in a given industry and location. If the cell is published, it might be possible for its local competitors to learn the receipts of the dominant firm to a high degree of precision.</p>
<p>Cell suppression rules based on this sort of reasoning are called <em>p</em>-percent rules, where <em>p</em> describes the precision with which the largest firm’s information can be learned. A conservative estimate of this occurs when the largest firm’s value is <em>(1-p)%</em> of the cell’s value.</p>
<p>A variant of this rule takes into account prior precision <em>q</em> (the “pq percent rule”). Another rule is known as the <em>n,k</em> rule: a cell is suppressed if <em>n</em> or fewer entities contribute <em>k</em> percent or more of the cell’s value. These rules are frequently applied to statistics produced by national statistical agencies <span class="citation" data-cites="harris-kojetin_statistical_2005">(Harris-Kojetin et al. <a href="ch024.xhtml#ref-harris-kojetin_statistical_2005" role="doc-biblioref">2005</a>)</span>. Simpler rules based entirely on cell counts are also encountered, for instance, in the Health and Retirement Study <span class="citation" data-cites="health_and_retirement_study_disclosure_nodate">(Health and Retirement Study <a href="ch024.xhtml#ref-health_and_retirement_study_disclosure_nodate" role="doc-biblioref">n.d.</a>)</span>. Tables produced using HRS confidential geo-coded data are only allowed to display values when the cell contains three or more records (five for marginal cells).</p>
<p>If a cell in a contingency table is suppressed based on any one of these rules, it’s original value could be backed out by using the information in the table margins and the understanding that table cells need to sum up to their margins. Some data providers therefore require that additional cells are suppressed to ensure this sort of reverse engineering is not possible. Figuring out how to choose these <em>complementary suppressions</em> in an efficient manner is a non-trivial challenge.</p>
<p>In general, cell suppression is not an ignorable form of SDL. It remains popular because it is easy to explain and does not affect the un-suppressed cells.</p>
<p>Data suppression is clearly non-ignorable, and it is quite difficult to correct for suppression in an SDL-aware analysis.<a href="#fn7" class="footnote-ref" id="fnref7" epub:type="noteref">7</a> The features of the data that lead to suppression are often related to the underlying phenomenon of interest. <span class="citation" data-cites="chetty_practical_2019">Chetty and Friedman (<a href="ch024.xhtml#ref-chetty_practical_2019" role="doc-biblioref">2019</a>)</span> provide a clear illustration. They publish neighborhood-level summaries of intergenerational mobility based on tax records linked to Census data. The underlying microdata are highly sensitive, and to protect privacy the researchers used a variant of a differentially privacy model. Chetty and Friedman show that if they had instead used a cell suppression rule, the published data would be misleading with respect to the relationship between neighborhood poverty and teen pregnancy, because both variables are associated with neighborhood population. Hence, the missingness induced by cell suppression is not ignorable.</p>
<p>Suppression can also be applied to model-based statistics. For instance, after having run a regression, coefficients that correspond to cells with fewer than <em>n</em> cases may be suppressed. This most often occurs when using dichotomous variables (dummy variables), which represent conditional means for particular subgroups.</p>
<div class="bboxfix">
<p>
In a regression, a researcher includes a set of dummies for interacting occupation and location. When cross-tabulating occupation and location, many cells have less than five observations contributing to the coefficient. The data provider requires that these be suppressed.
</p>
</div>
</section>
<section id="coarsening" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3"><span class="header-section-number">5.2.3</span> Coarsening</h3>
<p>Coarsening takes detailed attributes that can serve as quasi-identifiers and collapses them into a smaller number of categories. Computer scientists call this <em>generalizing</em>, and it is also sometimes referred to as <em>masking</em>. Coarsening can be applied to quasi-identifiers to prevent re-identification or to attributes to prevent accurate attribute inference. When applied to quasi-identifiers, the concern is that an outsider could use detailed quasi-identifiers to single-out a particular record and learn to whom it belonged. By coarsening quasi-identifiers, the set of matching records is increased, raising uncertainty about any re-identified individual’s true identity. In principle, all variables can serve as quasi-identifiers, and the concept of <em>k-anonymity</em> introduced by <span class="citation" data-cites="sweeney_achieving_2002">Sweeney (<a href="ch024.xhtml#ref-sweeney_achieving_2002" role="doc-biblioref">2002</a>)</span> is a useful framework for thinking about how to implement coarsening and other microdata SDL. <em>K-anonymity</em> is discussed in section <a href="#disclosure-risk">5.3.1</a>.</p>
<p>Coarsening is common in microdata releases. Generally, it may make sense to consider coarsening variables with heavy tails (earnings, payroll), residuals (truncate range, suppress labels of range). In public-use microdata from the American Community Survey, geographic areas are coarsened until all such areas represent at least 100,000 individuals <span class="citation" data-cites="us_census_bureau_finalpublic_2011">(U.S. Census Bureau <a href="ch024.xhtml#ref-us_census_bureau_finalpublic_2011" role="doc-biblioref">2011</a>)</span>. In many data sources, characteristics like age and income, are reported in bins even when the raw data are more detailed. Topcoding is a common type of coarsening in which variables, such as incomes above a certain threshold, are replaced with some topcoded value (e.g., US$200,000 in the Current Population Survey). When releasing model-based estimates, rounding (another form of coarsening) can satisfy statistical best practice (not releasing numbers beyond their statistical precision) as well as disclosure avoidance principles by preventing inferences that could be too precise about specific records in the data.</p>
<p>Whether coarsening is ignorable or not depends on the analysis to be performed. Consider the case in which incomes are topcoded above the 95th percentile. This form of SDL is ignorable with respect to estimating the 90th percentile of the income distribution (and all other quantiles below the 95th). However, coarsening age is not ignorable if the goal is to conduct an analysis of behavior of individuals around some age or date-of-birth cutoff. Coarsening rules should therefore bear in mind the intended analysis for the data and may be usefully paired with restricted-access protocols that allow trusted researchers access to the more detailed data. See <span class="citation" data-cites="burkhauser_estimating_2011">Burkhauser et al. (<a href="ch024.xhtml#ref-burkhauser_estimating_2011" role="doc-biblioref">2011</a>)</span> for an example of the impact of topcoding on estimates of earnings inequality.</p>
</section>
<section id="swapping" class="level3" data-number="5.2.4">
<h3 data-number="5.2.4"><span class="header-section-number">5.2.4</span> Swapping</h3>
<p>The premise behind the technique of <em>swapping</em> is similar to suppression. Again, each record is assigned a level of disclosure risk. Then any high-risk record is matched to a less risky record on a set of key variables, and all of the other non-key attributes are swapped. The result is a data set that preserves the distribution among all the key variables used for matching. If the original purpose of the data was to publish cross-tabulations of the matching variables, swapping can produce microdata that are consistent with those tabulations. This approach is more commonly used in censuses and surveys of people or households and rarely used with establishment data.</p>
<div class="bboxfix">
<p>
For example, consider the hypothetical health study again, and now suppose the known factors are Ann’s zip code, gender, race, ethnicity, age, smoking behavior, and the size of her household. Ann’s record might be classified as high risk if, for example, she has a very large household relative to the rest of the other respondents who are also from her zip code. If the data are used to publish summaries of smoking behavior by age, race, and gender, then Ann’s record would be matched to another record with the same age, race, gender, and smoking behavior, and the values of the household size and zip code attributes would be swapped.
</p>
</div>
<p>Swapping is ignorable for analyses that only depend on the matching variables, since the relationships among them will be preserved. However, swapping distorts relationships among the other variables and between the matching variables and the other variables. In the example above, the swapping would be non-ignorable in the context of a study of how smoking behavior varies across zip codes. In general, statistical agencies are not willing to publish detailed information about how swapping is implemented since that information could be used to reverse-engineer some of the swaps, undoing the protection. Hence, SDL-aware analysis may not be possible and inference validity negatively affected.</p>
</section>
<section id="sampling" class="level3" data-number="5.2.5">
<h3 data-number="5.2.5"><span class="header-section-number">5.2.5</span> Sampling</h3>
<p>Sampling is the original SDL technique. Rather than the full confidential microdata, publishing a sample inherently limits the certainty with which attackers can re-identify records. While sampling can provide a formal privacy guarantee, in modern, detailed surveys, sampling will not in general prevent re-identification. In combination with other tools, like coarsening, sampling may be particularly appealing because, while it is non-ignorable, researchers can adjust their analysis for the sampling using familiar methods. Sampling is often used in conjunction with other methods, including with formally private methods, to amplify the protection provided.</p>
</section>
<section id="noise-infusion" class="level3" data-number="5.2.6">
<h3 data-number="5.2.6"><span class="header-section-number">5.2.6</span> Noise Infusion</h3>
<p>Noise infusion can refer to an array of related methods, all of which involve distorting data with randomly distributed noise. There is a key distinction between methods where the microdata are infused with noise (input noise infusion), versus methods where noise is added to functions or aggregates of the data before publication (output noise infusion).</p>
<p>Noise infusion was developed as a substitute for cell suppression as an approach to protecting tabular summaries of business data. Originally proposed by <span class="citation" data-cites="evans_using_1998">Evans, Zayatz, and Slanta (<a href="ch024.xhtml#ref-evans_using_1998" role="doc-biblioref">1998</a>)</span>, the basic approach assigns each microdata unit (a business establishment) a multiplicative noise factor drawn from a symmetric distribution (e.g., centered on one) and multiplies sensitive (or all) characteristics by that factor. Tabular summaries can then be made from the distorted characteristics. As cell sizes increase, the distortions applied to each unit average out. Thus, while small cells may be quite distorted and thus protected, large cells usually have little distortion. Most cells no longer need to be suppressed. These approaches are used in the US Census Bureau’s Quarterly Workforce Indicators <span class="citation" data-cites="abowd_lehd_2009 abowd_dynamically_2012">(Abowd et al. <a href="ch024.xhtml#ref-abowd_lehd_2009" role="doc-biblioref">2009</a> ; J. M. Abowd, Gittings, et al. <a href="ch024.xhtml#ref-abowd_dynamically_2012" role="doc-biblioref">2012</a>)</span> and County Business Patterns with a truncated distribution. When the noise distribution is unbounded, for instance Gaussian, noise infusion may be differentially private (see chapter <a href="#diffpriv">6</a> on differential privacy).</p>
<p>Noise infusion has the advantage that it mostly eliminates the need to suppress sensitive records or cells, allowing more information to be revealed from the confidential data while maintaining certain confidentiality protections. Noise infusion also generally preserves the means and covariances among variables. However, it will always inflate estimated variances and can lead to bias in estimates of statistical models and in particular regression coefficients. Hence, noise infusion is generally not ignorable. If the details of the noise distribution can be made available to researchers, then it is possible to correct analysis for noise infusion. However, information about the noise distribution can also help an attacker reverse engineer the protections.</p>
</section>
<section id="synthetic-data-and-multiple-imputation" class="level3" data-number="5.2.7">
<h3 data-number="5.2.7"><span class="header-section-number">5.2.7</span> Synthetic Data and Multiple Imputation</h3>
<p>Synthetic data generation and multiple imputation are closely related. In fact, one particular variant of synthetic data as SDL (partially synthetic data) is also known as “suppress and impute” <span class="citation" data-cites="little_statistical_1993">(Little <a href="ch024.xhtml#ref-little_statistical_1993" role="doc-biblioref">1993</a>)</span>. Sensitive values for some or all records are replaced by (multiple) imputations. More generally, fully synthetic data <span class="citation" data-cites="rubin_discussion_1993">(Rubin <a href="ch024.xhtml#ref-rubin_discussion_1993" role="doc-biblioref">1993</a>)</span> replaces all values with draws from a posterior predictive distribution, estimated given the confidential data. For an overview, see <span class="citation" data-cites="raghunathan_multiple_2003">Raghunathan, Reiter, and Rubin (<a href="ch024.xhtml#ref-raghunathan_multiple_2003" role="doc-biblioref">2003</a>)</span>, <span class="citation" data-cites="little_statistical_2004">Little, Liu, and Raghunathan (<a href="ch024.xhtml#ref-little_statistical_2004" role="doc-biblioref">2004</a>)</span>, and <span class="citation" data-cites="drechsler_synthetic_2011">Drechsler (<a href="ch024.xhtml#ref-drechsler_synthetic_2011" role="doc-biblioref">2011</a>)</span>.</p>
<p>Synthetic data have been used in the Federal Reserve Board’s Survey of Consumer Finances to protect sensitive income values <span class="citation" data-cites="kennickell_multiple_1998">(Kennickell <a href="ch024.xhtml#ref-kennickell_multiple_1998" role="doc-biblioref">1998</a>)</span>, and in the US Census Bureau’s American Community Survey microdata to protect data from group quarters such as prisons and university residences <span class="citation" data-cites="hawala_disclosure_2009">(Hawala and Rodriguez <a href="ch024.xhtml#ref-hawala_disclosure_2009" role="doc-biblioref">2009</a>)</span>. The US Census Bureau’s LODES data, included in the <a href="https://onthemap.ces.census.gov/">OnTheMap</a> application, uses synthetic household data <span class="citation" data-cites="machanavajjhala_privacy_2008">(Machanavajjhala et al. <a href="ch024.xhtml#ref-machanavajjhala_privacy_2008" role="doc-biblioref">2008</a>)</span>. Synthetic data can be used in conjunction with validation servers: researchers use the synthetic data to create complex model-based estimation and then submit their analysis to a remote server with access to the confidential data for validation of the results. Such a mechanism has been used by the US Census Bureau in collaboration with Cornell University for confidential business microdata <span class="citation" data-cites="kinney_towards_2011">(Kinney et al. <a href="ch024.xhtml#ref-kinney_towards_2011" role="doc-biblioref">2011</a>)</span> and for survey data combined with administrative data <span class="citation" data-cites="abowd_final_2006">(Abowd, Stinson, and Benedetto <a href="ch024.xhtml#ref-abowd_final_2006" role="doc-biblioref">2006</a>)</span>. The term is sometimes used as well for test data for remote submission systems, which typically makes no claims as to the validity; it is simply constructed to replicate the data schema of the confidential data to test statistical code.</p>
</section>
<section id="examples-of-sdl-methods" class="level3" data-number="5.2.8">
<h3 data-number="5.2.8"><span class="header-section-number">5.2.8</span> Examples of SDL Methods</h3>
<p>Table <a href="#tab:overviewtable">5.1</a> shows how the various methods can be combined, drawing on examples both from this Handbook as well as from other frequently used data sources.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
Table 5.1: Summary of SDL methods
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
In this Hand- book
</th>
<th style="text-align:left;">
Remov- al of Direct Identi- fiers
</th>
<th style="text-align:left;">
Remov- al of Quasi- Identi- fiers
</th>
<th style="text-align:left;">
Supp- ress- ion
</th>
<th style="text-align:left;">
Coars- ening
</th>
<th style="text-align:left;">
Swapp- ing
</th>
<th style="text-align:left;">
Sampl- ing
</th>
<th style="text-align:left;">
Noise Infus- ion
</th>
<th style="text-align:left;">
Synth- etic Data
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
IAB On-Site Access
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
</tr>
<tr>
<td style="text-align:left;">
IAB Scientific Use Files
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
</tr>
<tr>
<td style="text-align:left;">
OLDA
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file14.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
</tr>
<tr>
<td style="text-align:left;">
NB-IRDT
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
—
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
</tr>
<tr>
<td style="text-align:left;">
PCRI
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
—
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
—
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
</tr>
<tr>
<td style="text-align:left;">
Aurora Public-Use File
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file14.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
</tr>
<tr>
<td style="text-align:left;">
Stanford-SFUSD
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file14.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
</tr>
<tr>
<td style="text-align:left;">
City of Cape Town
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file14.png" />
</td>
<td style="text-align:left;">
—
</td>
<td style="text-align:left;">
—
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
—
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
—
</td>
</tr>
<tr>
<td style="text-align:left;">
DIME (World Bank)
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file14.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
—
</td>
<td style="text-align:left;">
—
</td>
<td style="text-align:left;">
—
</td>
<td style="text-align:left;">
—
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
—
</td>
</tr>
<tr>
<td style="text-align:left;">
Survey of Consumer Finances
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
—
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
</tr>
<tr>
<td style="text-align:left;">
American Community Survey
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
—
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file14.png" />
</td>
</tr>
<tr>
<td style="text-align:left;">
Quarterly Workforce Indicators
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
<td style="text-align:left;">
<img src="../media/file12.png" />
</td>
<td style="text-align:left;">
<img src="../media/file13.png" />
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border: 0;" colspan="10">
<span style="font-style: italic;">Note: </span> <sup></sup> <img src="../media/file12.png" /> = Yes, <img src="../media/file14.png" /> = Partially, <img src="../media/file13.png" /> = No, — = No Info
</td>
</tr>
</tfoot>
</table>
</section>
</section>
<section id="metrics" class="level2" data-number="5.3">
<h2 data-number="5.3"><span class="header-section-number">5.3</span> Metrics</h2>
<!-- How do you measure risk, and the reduction in risk achieved by applying above methods? Mention uniqueness criteria, k-anonymity, l-diversity, matching metrics, etc. -->
<p>The design of an SDL system depends on determinations about what constitutes an acceptable level of disclosure risk, balanced with the proposed uses of the data. There are many different ways to describe and measure disclosure risk. A commonality these systems share is the ability to determine the uniqueness of a record, or combination of attributes in the data, that then intuitively predicts the ease with which a record could be distinguished to re-identify the respondent (perhaps aided by a linked data set). Likewise, there are many different ways to assess whether the released data are suitable, or fit, for their intended use. These quality measures are often based on how closely the released data match the true data on certain statistical summaries, and it will be important for researchers and data custodians to agree on what are the most relevant summaries.</p>
<section id="disclosure-risk" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1"><span class="header-section-number">5.3.1</span> Disclosure Risk</h3>
<p>Early definitions of disclosure risk were based on rules and guidelines derived from institutional knowledge, assessment of summary measures, and re-identification experiments <span class="citation" data-cites="harris-kojetin_statistical_2005">(Harris-Kojetin et al. <a href="ch024.xhtml#ref-harris-kojetin_statistical_2005" role="doc-biblioref">2005</a>)</span>. Statisticians have subsequently developed more formal models to measure risk of re-identification for specific types of publication and with particular threat models. For instance, <span class="citation" data-cites="shlomo_assessing_2010">Shlomo and Skinner (<a href="ch024.xhtml#ref-shlomo_assessing_2010" role="doc-biblioref">2010</a>)</span> model re-identification risk in survey microdata when an attacker is matching on certain categorical variables.</p>
<p>Recently, computer scientists and statisticians have introduced more general concepts of disclosure risk and data privacy. Latanya Sweeney proposed the concept of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-anonymity <span class="citation" data-cites="sweeney_achieving_2002">(Sweeney <a href="ch024.xhtml#ref-sweeney_achieving_2002" role="doc-biblioref">2002</a>)</span> which defines disclosure risk in terms of the number of records that share the same combination of attributes. If a single record is uniquely identified by some combination of attributes, disclosure risk is high. Sweeney says that a data set can be called <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-anonymous if for all feasible combinations of attributes, at least <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> records have that combination. Intuitively, increases in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> reduce the risk that observations can be singled out by linking other data sets that contain the same attributes. The concept of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-anonymity can provide some guidance when thinking about how to implement the SDL systems described above. For example, if records are uniquely identified by age, race, and gender, then one might collapse age into brackets until there are at least <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k&gt;1</annotation></semantics></math> records for each such combination.</p>
<p>However, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-anonymity does not protect against attribute disclosure. If all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> observations with the same combination of attributes also share the same sensitive attribute, for example, smoking behavior, then the published data do not fully prevent disclosure of smoking behavior. Recognizing this, <span class="citation" data-cites="machanavajjhala_l-diversity_2007">Machanavajjhala et al. (<a href="ch024.xhtml#ref-machanavajjhala_l-diversity_2007" role="doc-biblioref">2007</a>)</span> introduce the concept of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℓ</mi><annotation encoding="application/x-tex">\ell</annotation></semantics></math>-diversity. The idea is that whenever a group of records are identical on some set of variables, there must be a certain amount of heterogeneity in important sensitive traits. If a certain group of records matches on a set of quasi-identifiers and also all share the same smoking status, then to achieve <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℓ</mi><annotation encoding="application/x-tex">\ell</annotation></semantics></math>-diversity, one might alter the reported smoking behavior of some fraction (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℓ</mi><annotation encoding="application/x-tex">\ell</annotation></semantics></math>) of the records—a form of noise infusion.</p>
</section>
<section id="data-quality" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2"><span class="header-section-number">5.3.2</span> Data Quality</h3>
<p>When the released data or output are tabular (histograms, cross-tabulations) or are a limited set of population or model parameters (means, coefficients), a set of distance-based metrics (so-called “<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ℓ</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\ell_p</annotation></semantics></math> distance” metrics) can be used to compare the quality of the perturbed data. Note that this is a specific metric, as it is limited to those statistics taken into account—the data quality may be very poor in non-measured attributes! For <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p=1</annotation></semantics></math>, the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ℓ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\ell_1</annotation></semantics></math> distance is the sum of absolute differences between the confidential and perturbed data. For <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">p = 2</annotation></semantics></math>, the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ℓ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\ell_2</annotation></semantics></math> distance is the sum of squared differences between the two data sets (normalized by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> the number of observations, it is the Mean Squared Error, MSE).</p>
<p>In settings where it is important to measure data quality over an entire distribution, the Kullback-Leibler (KL) divergence measure can also be used. The KL-divergence is related to the concept of entropy from information theory and, loosely, measures the amount of surprise associated with seeing an observation drawn from one distribution when one expected them to come from another distribution. Other metrics are based on propensity scores <span class="citation" data-cites="woo_global_2009 snoke_general_2018">(Woo et al. <a href="ch024.xhtml#ref-woo_global_2009" role="doc-biblioref">2009</a>; Snoke et al. <a href="ch024.xhtml#ref-snoke_general_2018" role="doc-biblioref">2018</a>)</span>. More specific measures will often compare specific analysis output, a task that is quite difficult to conduct in general. <span class="citation" data-cites="reiter_verification_2009">Reiter, Oganian, and Karr (<a href="ch024.xhtml#ref-reiter_verification_2009" role="doc-biblioref">2009</a>)</span> propose to summarize the difference between regression coefficients when analyses can be run on both confidential and protected data in the context of verification servers.</p>
</section>
</section>
<section id="tools" class="level2" data-number="5.4">
<h2 data-number="5.4"><span class="header-section-number">5.4</span> Tools</h2>
<p>For data providers faced with the need to start providing safe data for use by external researchers, a growing number of software packages are available that implement the methods described in this chapter. The Inter-university Consortium for Political and Social Research (ICPSR) has a checklist that may be of use in early development of an SDL system <span class="citation" data-cites="icpsr_disclosure_2020">(ICPSR <a href="ch024.xhtml#ref-icpsr_disclosure_2020" role="doc-biblioref">2020</a>)</span>. The listing of tools below is incomplete but will provide practitioners with a place to start. A fully developed SDL system will have unique requirements and may require custom programming. Nevertheless, many tools are useful across a wide range of applications.</p>
<p>Statistics Netherlands maintains the ARGUS software for SDL <span class="citation" data-cites="hundepool_argus_1998">(Hundepool and Willenborg <a href="ch024.xhtml#ref-hundepool_argus_1998" role="doc-biblioref">1998</a>)</span>, including τ-ARGUS to protect tabular data <span class="citation" data-cites="de_wolf_-argus_2018">(De Wolf <a href="ch024.xhtml#ref-de_wolf_-argus_2018" role="doc-biblioref">2018</a>)</span>, and μ-ARGUS for protecting microdata <span class="citation" data-cites="hundepool_-argus_2018">(Hundepool and Ramaswamy <a href="ch024.xhtml#ref-hundepool_-argus_2018" role="doc-biblioref">2018</a>)</span>. The software appears to be widely used in statistical agencies in Europe. An open-source R package, <code>sdcMicro</code>, implements a full suite of tools needed to apply SDL, from computation of risk measures, including <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-anonymity and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℓ</mi><annotation encoding="application/x-tex">\ell</annotation></semantics></math>-diversity, to implementation of SDL methods and the computation of data quality measures <span class="citation" data-cites="templ_statistical_2015 templ_sdcmicro_2020">(Templ, Kowarik, and Meindl <a href="ch024.xhtml#ref-templ_statistical_2015" role="doc-biblioref">2015</a>; Templ, Meindl, and Kowarik <a href="ch024.xhtml#ref-templ_sdcmicro_2020" role="doc-biblioref">2020</a>)</span>.</p>
<p>Simpler tools, focusing on removing direct identifiers, can be found at J-PAL for Stata (<a href="https://github.com/J-PAL/stata_PII_scan">stata_PII_scan</a>) and R (<a href="https://github.com/J-PAL/PII-Scan">PII-scan</a>), and at Innovations for Poverty Action (IPA) for Python or Windows (<a href="https://github.com/PovertyAction/PII_detection">PII_detection</a>) <span class="citation" data-cites="j-pal_j-palstata_pii_scan_2020 j-pal_j-palpii-scan_2020 innovations_for_poverty_action_povertyactionpii_detection_2020">(J-PAL <a href="ch024.xhtml#ref-j-pal_j-palstata_pii_scan_2020" role="doc-biblioref">2020</a><a href="ch024.xhtml#ref-j-pal_j-palstata_pii_scan_2020" role="doc-biblioref">b</a>, <a href="ch024.xhtml#ref-j-pal_j-palpii-scan_2020" role="doc-biblioref">2020</a><a href="ch024.xhtml#ref-j-pal_j-palpii-scan_2020" role="doc-biblioref">a</a>; Innovations for Poverty Action <a href="ch024.xhtml#ref-innovations_for_poverty_action_povertyactionpii_detection_2020" role="doc-biblioref">2020</a>)</span>.</p>
<p>A number of R packages facilitate generation of synthetic data. <span class="citation" data-cites="raab_practical_2016">Raab, Nowok, and Dibben (<a href="ch024.xhtml#ref-raab_practical_2016" role="doc-biblioref">2016</a>)</span> and <span class="citation" data-cites="nowok_synthpop_2016">Nowok, Raab, and Dibben (<a href="ch024.xhtml#ref-nowok_synthpop_2016" role="doc-biblioref">2016</a>)</span> provide <code>synthpop</code>, a flexible and up-to-date package with methods for generating synthetic microdata. The R package <code>simPop</code> <span class="citation" data-cites="templ_simpop_2019">(Templ et al. <a href="ch024.xhtml#ref-templ_simpop_2019" role="doc-biblioref">2019</a>)</span> can also generate synthetic populations from aggregate data, which can be useful for testing SDL systems on non-sensitive data. In some cases, one might also consider using general-purpose software for multiple imputation for data synthesis.<a href="#fn8" class="footnote-ref" id="fnref8" epub:type="noteref">8</a></p>
<p>Many of the methods described in this chapter are technical and require statistical and programming expertise. If that expertise is not already available among staff, some institutions provide guidance to researchers who wish to apply SDL techniques.</p>
</section>
<section id="conclusion" class="level2" data-number="5.5">
<h2 data-number="5.5"><span class="header-section-number">5.5</span> Conclusion</h2>
<p>There is now a greater demand for all kinds of data. More than ever before, scholars and analysts have the tools to use data to better understand the economy and society and to inform policy. Alongside these advances, data custodians find themselves under pressure to make databases available to outsiders. However, the pressure to make data available is not always accompanied by the resources, tools, or expertise needed to do so safely.</p>
<p>The same advances driving these new demands have a darker side. Computing power together with the availability of detailed outside data make it easier than ever for attackers to exploit improperly protected data. Therefore, when making data available for research, agency stewards must take great care to also protect the subjects in the data. This chapter provides an overview of techniques traditionally used to modify the data to achieve that goal. There is a legitimate concern that some of the methods discussed here cannot protect against all possible attacks made possible with modern computing power. Those concerns animate the discussion of formal methods that yield provable privacy guarantees elsewhere in this Handbook.</p>
</section>
<section id="about-the-authors-1" class="level2 unnumbered" data-number="">
<h2 class="unnumbered" data-number="">About the Authors</h2>
<p><a href="http://ianschmutte.org/">Ian M. Schmutte</a> is Associate Professor in the Department of Economics at the University of Georgia. Ian is currently working with the US Census Bureau on new methods for protecting confidential data. His research has appeared in the <em>American Economic Review</em>, <em>Journal of Labor Economics</em>, <em>Journal of Human Resources</em>, <em>Journal of Business &amp; Economic Statistics</em>, and the <em>Brookings Papers on Economic Activity</em>.</p>
<p><a href="https://lars.vilhuber.com/">Lars Vilhuber</a> is the Executive Director of the Labor Dynamics Institute at Cornell University, and a faculty member in Cornell University’s Economics Department. He is also the American Economic Association’s Data Editor. Lars is a Co-Chair of IDEA. His research interests relate to the dynamics of the labor market. He also has extensive experience in the application of privacy-preserving publication and access to restricted data. He is chair of the scientific committee of the French restricted-access system <a href="https://casd.eu">CASD</a>, member of the governing board of the Canadian Research Data Centre Network (<a href="https://crdcn.org">CRDCN</a>), and incoming chair of the American Statistical Association‘s <a href="https://community.amstat.org/cpc/home">Committee on Privacy and Confidentiality</a>. Lars has an undergraduate degree in Economics from Universität Bonn and a Ph.D. in Economics from Université de Montréal.</p>
</section>
<section id="disclaimer" class="level2 unnumbered" data-number="">
<h2 class="unnumbered" data-number="">Disclaimer</h2>
<p>The views expressed in this paper are those of the authors and not those of the US Census Bureau or other sponsors.</p>
</section>
<section id="acknowledgements" class="level2 unnumbered" data-number="">
<h2 class="unnumbered" data-number="">Acknowledgements</h2>
<p>This chapter draws on <span class="citation" data-cites="abowd_introductory_2019">J. M. Abowd, Schmutte, et al. (<a href="ch024.xhtml#ref-abowd_introductory_2019" role="doc-biblioref">2019</a>)</span> and the INFO7470 class at Cornell University <span class="citation" data-cites="abowd_session_2016">(Abowd and Vilhuber <a href="ch024.xhtml#ref-abowd_session_2016" role="doc-biblioref">2016</a>)</span>. We gratefully acknowledge the support of Alfred P. <a href="https://sloan.org/grant-detail/6845">Sloan Foundation Grant G-2015-13903</a> and <a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=1131848">National Science Foundation (NSF) Grant SES-1131848</a> for the earlier work.</p>
<!--chapter:end:05_00_discavoid.Rmd-->
</section>
</section>
<section class="footnotes" epub:type="footnotes">
<hr />
<ol>
<li id="fn1" epub:type="footnote"><p>Other terms sometimes used are “anonymization” or “de-identification,” but as this chapter will show, de-identification is a particular method of SDL, and anonymization is a goal, never fully achieved, rather than a method.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" epub:type="footnote"><p>In the United States, 62% of individuals are aware (and possibly resigned) that government and private companies collect data on them, and seem to believe that there is little benefit to them of such collection: 81% think so when companies do the data collection, and 66% when the government does so <span class="citation" data-cites="auxier_americans_2019">(Auxier et al. <a href="ch024.xhtml#ref-auxier_americans_2019" role="doc-biblioref">2019</a>)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" epub:type="footnote"><p>There is a large and robust literature in economics on the value of privacy. For an overview of ideas in this literature, we recommend <span class="citation" data-cites="varian_economic_2002">Varian (<a href="ch024.xhtml#ref-varian_economic_2002" role="doc-biblioref">2002</a>)</span> and <span class="citation" data-cites="acquisti_economics_2016">Acquisti, Taylor, and Wagman (<a href="ch024.xhtml#ref-acquisti_economics_2016" role="doc-biblioref">2016</a>)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" epub:type="footnote"><p>Thus the occasional reference to methods as <em>de-identification</em> or <em>anonymization</em>, though these terms can sometimes be misleading in regard to what they can actually achieve.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" epub:type="footnote"><p>As of the writing of this chapter in August 2020, WP22 is being revised and updated, but has not yet been published.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" epub:type="footnote"><p>See guidance in <span class="citation" data-cites="world_bank_dime_nodate">World Bank (<a href="ch024.xhtml#ref-world_bank_dime_nodate" role="doc-biblioref">n.d.</a>)</span> and <span class="citation" data-cites="kopper_j-pal_2020">Kopper, Sautmann, and Turitto (<a href="ch024.xhtml#ref-kopper_j-pal_2020" role="doc-biblioref">2020</a>)</span>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" epub:type="footnote"><p>One approach is to replace suppressed cells with imputed values, and then treat the data as multiply-imputed.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" epub:type="footnote"><p>See “<a href="https://stats.idre.ucla.edu/stata/seminars/mi_in_stata_pt1_new/">Multiple imputation in Stata</a>” or the <code>mice</code> package in R <span class="citation" data-cites="buuren_mice_2011">(Buuren and Groothuis-Oudshoorn <a href="ch024.xhtml#ref-buuren_mice_2011" role="doc-biblioref">2011</a>)</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
