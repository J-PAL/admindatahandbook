<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Ian M. Schmutte and Lars Vilhuber" />

<meta name="date" content="2020-08-31" />

<title>Prepared for handbook</title>

<script src="_main_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="_main_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="_main_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="_main_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="_main_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="_main_files/navigation-1.1/tabsets.js"></script>
<link href="_main_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="_main_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Prepared for handbook</h1>
<h4 class="author">Ian M. Schmutte and Lars Vilhuber</h4>
<h4 class="date">2020-08-31</h4>

</div>

<div id="TOC">
<ul>
<li><a href="#overview-of-disclosure-avoidance-methods"><span class="toc-section-number">0.1</span> Overview of disclosure avoidance methods</a><ul>
<li><a href="#purpose-of-statistical-disclosure-limitation-methods-definitions-and-context"><span class="toc-section-number">0.1.1</span> Purpose of statistical disclosure limitation methods: Definitions and Context</a></li>
<li><a href="#methods"><span class="toc-section-number">0.1.2</span> Methods</a></li>
<li><a href="#metrics"><span class="toc-section-number">0.1.3</span> Metrics</a></li>
<li><a href="#tools"><span class="toc-section-number">0.1.4</span> Tools</a></li>
<li><a href="#conclusion"><span class="toc-section-number">0.1.5</span> Conclusion</a></li>
<li><a href="#about-the-authors"><span class="toc-section-number">0.1.6</span> About the Authors</a></li>
<li><a href="#acknowledgements"><span class="toc-section-number">0.1.7</span> Acknowledgements</a></li>
<li><a href="#disclaimer"><span class="toc-section-number">0.1.8</span> Disclaimer</a></li>
<li><a href="#references"><span class="toc-section-number">0.1.9</span> References</a></li>
</ul></li>
</ul>
</div>

<div id="overview-of-disclosure-avoidance-methods" class="section level2">
<h2><span class="header-section-number">0.1</span> Overview of disclosure avoidance methods</h2>
<p>The purpose of this handbook is to provide guidance on how to enable broader but ethical and legal access to data. Data providers need to create “safe data” that can be provided to trusted “safe users”, within “<a href="physical%20safeguards">safe settings</a>”, subject to legal and contractual safeguards. Related, but distinct, is the question of how to create “safe outputs” from researchers’ findings, before those findings finally make their way into the public through, say, policy briefs or the academic literature. The processes used to create “safe data” and “safe outputs” - manipulations that render data less sensitive and therefore more appropriate for public release - are generally referred to as ||statistical disclosure limitation|| (SDL).<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> In this chapter, we will describe methods traditionally used within the field of SDL, pointing at methods as well as metrics to assess the resultant statistical quality and sensitivity of the data. Newer methods, generally referred to as “formal privacy methods,” are described in a separate chapter (Nissim et al NEED REF).</p>
<p>At their core, SDL methods prevent outsiders from learning “too much” about any one record in the data <span class="citation">(Dalenius, <a href="#ref-dalenius_towards_1977" role="doc-biblioref">1977</a>)</span> by deliberately, and judiciously, adding distortions. Ideally, these distortions maintain the validity of the data for statistical analysis, but strongly reduce the ability to isolate records and infer precise information about individual people, firms, or cases. In general, it is necessary to sacrifice validity in order to prevent disclosure <span class="citation">(Abowd &amp; Schmutte, <a href="#ref-abowd_economic_2015" role="doc-biblioref">2015</a>; Goroff, <a href="#ref-goroff_balancing_2015" role="doc-biblioref">2015</a>)</span>. It is therefore important for data custodians to bear this tradeoff in mind when deciding whether and how to use SDL.</p>
<p>One key challenge for implementing privacy systems lies in choosing the amount, or type, of privacy to provide. Answering this question requires some way to understand the individual and social value of privacy. <span class="citation">J. Abowd &amp; Schmutte (<a href="#ref-abowd_economic_2019" role="doc-biblioref">2019</a>)</span> discuss the question of optimal privacy protection (see also <span class="citation">Hsu et al. (<a href="#ref-hsu_differential_2014" role="doc-biblioref">2014</a>)</span> in the specific context of differential privacy). For an illustration, see <span class="citation">Spencer &amp; Seeskin (<a href="#ref-spencer_effects_2015" role="doc-biblioref">2015</a>)</span>, who use a calibration exercise to study the costs, measured in misallocated congressional seats, of reduced accuracy in population census data.</p>
<p>Part of the social value of privacy arises from its relationship to scientific integrity. While the law of information recovery suggests that improved privacy must come at the cost of increased error in published statistics, these effects might be mitigated through two distinct channels. First, people may be more truthful in surveys if they believe their data is not at risk, as <span class="citation">Couper, Singer, Conrad, &amp; Groves (<a href="#ref-couper_risk_2008" role="doc-biblioref">2008</a>)</span> illustrate. Second, work in computer science and statistics <span class="citation">(Dwork et al., <a href="#ref-dwork_generalization_2015" role="doc-biblioref">2015</a>, pp. @dwork_fienberg_2018, @cummings_adaptive_2016)</span> suggests a somewhat surprising benefit of differential privacy: protection against overfitting.</p>
<p>There are three factors that a data custodian should bear in mind when deciding whether and how to implement an SDL system in support of making data accessible. First, it is necessary to clarify the specific privacy requirements based on the nature of the underlying data, institutional and policy criteria, and ethical considerations. In addition, the custodian, perhaps in consultation with users, should clarify what sorts of analyses the data will support. Finally, SDL is always to be used as part of a broader system involving access restrictions and other technical barriers. The broader system may allow for less stringent SDL techniques when providing data to researchers in secure environments than would be possible if data were to be released as unrestricted public use data.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> This implies in particular that we will not provide a recommendation for a “best” method, since no such globally optimal method exists in isolation.</p>
<p>Rather, we provide here an overview of the concepts behind SDL and some of the more widely-used methods. Our hope is to provide a reference point from which data providers and data users can discuss which forms of SDL are appropriate and satisfy the needs of both parties. In particular, we will focus on how common SDL tools affect different types of statistical analysis as well as the kind of confidentiality protections they support, drawing heavily on <span class="citation">Abowd &amp; Schmutte (<a href="#ref-abowd_economic_2015" role="doc-biblioref">2015</a>)</span>. SDL is a broad topic with a vast literature, starting with <span class="citation">Fellegi (<a href="#ref-fellegi_question_1972" role="doc-biblioref">1972</a>)</span>. Naturally, this brief summary is not a replacement for the textbook treatment of SDL in <span class="citation">Duncan, Elliot, &amp; Salazar-González (<a href="#ref-duncan_statistical_2011" role="doc-biblioref">2011</a>)</span>. Finally, SDL methods must be implemented and deployed, and we provide pointers to existing “off-the-rack” tools in a variety of platforms (Stata, R, and Python). Readers might also consult other summaries and guides, such as <span class="citation">Dupriez &amp; Boyko (<a href="#ref-dupriez_dissemination_2010" role="doc-biblioref">2010</a>)</span>, <span class="citation">World Bank (<a href="#ref-world_bank_dime_nodate" role="doc-biblioref">n.d.</a>)</span>, <span class="citation">Kopper, Sautmann, &amp; Turitto (<a href="#ref-kopper_j-pal_2020" role="doc-biblioref">2020</a>)</span>, and <span class="citation">Liu (<a href="#ref-liu_statistical_2020" role="doc-biblioref">2020</a>)</span>.</p>
<div id="purpose-of-statistical-disclosure-limitation-methods-definitions-and-context" class="section level3">
<h3><span class="header-section-number">0.1.1</span> Purpose of statistical disclosure limitation methods: Definitions and Context</h3>
<!-- (1-1.5 pages) -->
<p>A clear and precise sense of what constitutes an unauthorized disclosure is a prerequisite to implementing SDL. Are all data items equally sensitive? How much more should one be able to learn about certain classes of people, firms, villages, etc.? Note that even when “trusted researchers” (“safe people”) can be sworn to secrecy, the ultimate goal is to publish using information gleaned from the data, and the final audience can never be considered trusted.</p>
<p>The key concepts are privacy and confidentiality. ||Privacy|| can be viewed, in this context, as the right to restrict others’ access to personal information, whether through query or through observation <span class="citation">(Hirshleifer, <a href="#ref-hirshleifer_privacy_1980" role="doc-biblioref">1980</a>)</span>. ||Confidentiality|| pertains to data that have already been collected, and describes the principle that the data should not be used in ways that could harm the persons that provided it.</p>
<blockquote>
<p>For example, Ann, who is asked to participate in a study about health behaviors, has a <em>privacy</em> right to refuse to answer a question about smoking. If she does answer the question, it would breach <em>confidentiality</em> if her response was then used by an insurance company to adjust her premiums <span class="citation">(Duncan, Jabine, &amp; Wolf, <a href="#ref-duncan_private_1993" role="doc-biblioref">1993</a>)</span>.</p>
</blockquote>
<p><span class="citation">Harris-Kojetin et al. (<a href="#ref-harris-kojetin_statistical_2005" role="doc-biblioref">2005</a>)</span> define “disclosure” as the “inappropriate attribution of information to a data subject, whether an individual or an organization” <span class="citation">(Harris-Kojetin et al., <a href="#ref-harris-kojetin_statistical_2005" role="doc-biblioref">2005</a>, p. 4)</span>, and describe three different types of disclosure. An <em>identity disclosure</em> is one where it is possible to learn that a particular record or data item belongs to a particular participant (individual or organization). An <em>attribute disclosure</em> happens if publication of the data reveals an attribute of a participant. Note that an <em>identity disclosure</em> necessarily entails attribute disclosure, but the reverse is not the case.</p>
<blockquote>
<p>In our hypothetical health study, if Ann responds that she is a smoker, an identity disclosure would mean someone can determine which record is hers, and therefore can also learn that she is a smoker – an attribute disclosure. However, an attribute disclosure could also occur if someone knows that Ann was in the study, they know that Ann lives in a particular zip code, and the data reveal that all participants from that zip code are also smokers. Her full record is not revealed, but confidentiality was breached all the same.</p>
</blockquote>
<p>With these concepts in mind, it is necessary to ask whether it is sufficient to prevent blatant “all-or-nothing” identity or attribute disclosures. Usually not, as it may be possible to learn a sensitive attribute with high, but not total, certainty. This is called an <em>inferential disclosure</em> <span class="citation">(Dalenius, <a href="#ref-dalenius_towards_1977" role="doc-biblioref">1977</a>; Duncan &amp; Lambert, <a href="#ref-duncan_disclosure-limited_1986" role="doc-biblioref">1986</a>)</span>.</p>
<blockquote>
<p>Suppose Ann’s health insurer knows that Ann is in the data, and that she lives in a particular zip code. If the data have 100 records from that zip code and 99 are smokers, then the insurer has learned Ann’s smoking status with imperfect, but high precision.</p>
</blockquote>
<p>In addition to deciding what kinds of disclosure can be tolerated and to what extent, in many cases it may also be meaningful to decide which characteristics are and are not sensitive. Smoking behavior may nowadays be regarded as sensitive, but depending on the context, gender might not be. Or, in the case of business data, total sales volume or total payroll are highly sensitive trade secrets. Generally, the county in which the business is located, or the industry in which it operates might not be, but consider a survey of self-employed business people - the location of the business might be the home address, which might be considered highly sensitive. These decisions on what is sensitive affect the implementation of a privacy protection system.</p>
<p>However, additional care must be taken because variables that are not inherently sensitive can still be used to isolate and identify records. Such variables are sometimes referred to as ||<em>quasi-identifiers</em>|| and they can be exploited for ||<em>reidentification</em>|| attacks. In business data, if the data show that there is only one firm operating in a particular county and sector, then their presence inherently leads to identity disclosure. Many of the traditional approaches to SDL operate in large part by preventing re-identification.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> <span class="citation">Garfinkel (<a href="#ref-garfinkel_-identification_2015" role="doc-biblioref">2015</a>)</span> discusses techniques for de-identifying data and the many ways in which modern computing tools and a data-rich environment may render effective de-identification impossible, reinforcing the growing need for formal privacy models like differential privacy.</p>
<!-- > Confidential  should  mean  that  the  dissemination  of  data  in  a  manner  that  would  allow  public identification of the respondent or would in any way be harmful to him is prohibited and that the data are immune from legal process.  (SOURCE?) -->
<p>In the United States, 62% of individuals are aware (and possibly resigned) that government and private companies collect data on them, and seem to believe that there is little benefit to them of such collection: 81% think so when companies do the data collection, and 66% when the government does so <span class="citation">(Auxier et al., <a href="#ref-auxier_americans_2019" role="doc-biblioref">2019</a>)</span>.</p>
<p>There is a large and robust literature on the value of privacy in economics. However, that literature is generally focused on the value to individuals of being able to conceal private information, like a health condition, from a firm or prospective employer. The challenge to the firm is to design mechanisms, like pricing strategies, that encourage people to disclose private information. For an overview of ideas in this literature, we recommend reading <span class="citation">Stigler (<a href="#ref-stigler_introduction_1980" role="doc-biblioref">1980</a>)</span>, <span class="citation">Posner (<a href="#ref-posner_economics_1981" role="doc-biblioref">1981</a>)</span>, and <span class="citation">Hirshleifer (<a href="#ref-hirshleifer_privacy_1980" role="doc-biblioref">1980</a>)</span>. <span class="citation">Varian (<a href="#ref-varian_economic_2002" role="doc-biblioref">2002</a>)</span> and <span class="citation">Acquisti, Taylor, &amp; Wagman (<a href="#ref-acquisti_economics_2016" role="doc-biblioref">2016</a>)</span> both provide comprehensive surveys at different points in the development of this literature.</p>
<p>SDL methods may be required for legal and ethical reasons. ||Institutional Review Boards|| require that individual’s well-being be protected (see [chapter on IRB]). Legal mandates may intersect with ethical concerns, or prescribe certain (minimal) criteria. Thus, the U.S. Health Insurance Portability and Accountability Act of 1996 (HIPAA) <span class="citation">(U.S. Department of Health &amp; Human Services, <a href="#ref-us_department_of_health__human_services_health_nodate" role="doc-biblioref">n.d.</a>)</span> has precise definitions of variables that need to be removed in order to comply with the law’s mandate of deidentification <span class="citation">(Department of Health and Human Services, <a href="#ref-department_of_health_and_human_services_methods_2012" role="doc-biblioref">2012</a>)</span>. The European ||General Data Protection Regulation|| (GDPR) came into effect in 2018, and has defined both broadly the way researchers can access data, and more narrowly the requirements for disclosure limitation [<span class="citation">Cohen &amp; Nissim (<a href="#ref-cohen_towards_2020" role="doc-biblioref">2020</a>)</span>;greene_adjusting_2019;molnar-gabor_germany_2018]. Similar laws are emerging around the world, and will define both minimal requirements and limits of SDL and other access controls. The ||California Consumer Privacy Act|| (CCPA) <span class="citation">(Marini, Kateifides, &amp; Bates, <a href="#ref-marini_comparing_2018" role="doc-biblioref">2018</a>)</span> and the Brazilian ||Lei Geral de Proteção de Dados|| (LGDP) <span class="citation">(Black, Ramos, &amp; Biscardi, <a href="#ref-black_6_2020" role="doc-biblioref">2020</a>)</span> came into effect in 2020, and India is currently considering such a law <span class="citation">(Panakal, <a href="#ref-panakal_indias_2019" role="doc-biblioref">2019</a>)</span>.</p>
</div>
<div id="methods" class="section level3">
<h3><span class="header-section-number">0.1.2</span> Methods</h3>
<p>There are many different SDL methods, and the decision of which to use depends on what needs to be protected, how their use will affect approved analyses, and their technical properties. At a very high level, we can think of an SDL system as a mechanism that takes the raw confidential data, <span class="math inline">\(D\)</span>, as an input and produces a modified dataset, <span class="math inline">\(\tilde{D}\)</span>. The researcher then conducts their analysis with the modified <span class="math inline">\(\tilde{D}\)</span>. Ideally, the researcher can do their analysis as planned, but the risk of disclosure in <span class="math inline">\(\tilde{D}\)</span> is reduced.</p>
<p>Researchers generally need to consider all of the design features that went into producing the data used for an analysis. Most already do so in the context of surveys, where design measures are incorporated into the analysis, often directly in software packages. Some of these adjustments may already take into account various SDL techniques. Traditional survey design adjustments can take into account <a href="#sampling">sampling</a>. Some forms of <a href="#coarsening">coarsening</a> may already be amenable to adjustment using various clustering techniques, such as <span class="citation">Moulton (<a href="#ref-moulton_random_1986" role="doc-biblioref">1986</a>)</span> (see <span class="citation">Cameron &amp; Miller (<a href="#ref-cameron_practitioners_2015" role="doc-biblioref">2015</a>)</span>).</p>
<p>More generally, the inclusion of edits to the data done in service of disclosure limitation is less well supported, and less well integrated into standard research methods. <span class="citation">Abowd &amp; Schmutte (<a href="#ref-abowd_economic_2015" role="doc-biblioref">2015</a>)</span> argue that the analysis of SDL-laden data is inherently compromised because the details of the SDL protections cannot be disclosed. If they cannot be disclosed, their consequences for inference are unknowable, and, as they show, may be substantial. Regression models, regression discontinuity designs, and instrumental variables models are, generally speaking, affected when SDL is present. The exact nature of any bias or inconsistency will depend on whether SDL was applied to explanatory variables, dependent variables, instruments, or all of the above. Furthermore, it is not always the case that SDL induces an attenuating bias.</p>
<p>With these goals in mind, following <span class="citation">Abowd &amp; Schmutte (<a href="#ref-abowd_economic_2015" role="doc-biblioref">2015</a>)</span> we distinguish between <em>ignorable</em> and <em>non-ignorable</em> SDL systems. Briefly, SDL is <em>ignorable</em> for a particular analysis if the analysis can be performed on the modified data, <span class="math inline">\(\tilde{D}\)</span> as though it were the true data. In a non-ignorable analysis, the result differs in some material way when <span class="math inline">\(\tilde{D}\)</span> is substituted for <span class="math inline">\(D\)</span>. When the SDL method is <em>known</em>, then it may be possible for the researcher to perform an <em>SDL-aware</em> analysis that corrects for non-ignorability. However, SDL methods are almost never ignorable. Non-ignorable SDL may be discoverable if and only if the analysis of the published data can be probabilistically corrected for the data alterations introduced by the SDL. Many SDL methods have this latter property.</p>
<p>We briefly outline several of the methods most commonly used within national statistical offices. For interested readers, <span class="citation">Harris-Kojetin et al. (<a href="#ref-harris-kojetin_statistical_2005" role="doc-biblioref">2005</a>)</span> describe how SDL systems are implemented in the U.S. statistical system,<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> while <span class="citation">Dupriez &amp; Boyko (<a href="#ref-dupriez_dissemination_2010" role="doc-biblioref">2010</a>)</span> offers a more multinational perspective.</p>
<div id="de-identification" class="section level4">
<h4><span class="header-section-number">0.1.2.1</span> De-identification</h4>
<p>In general, it is good practice to remove any variables from the data that are not needed for data processing or analysis, and that could be considered direct identifiers. What constitutes “direct identifiers” may differ on the context, but generally comprises any variable that might directly link to confidential information: names, account or identifier numbers, and sometimes exact birth dates or exact geo-identifiers.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> HIPAA defines sixteen identifiers that must be removed in order to comply with the law. It may be necessary to preserve identifiers through parts of the data processing or analysis if they are key variables needed for record linking. For instance, names may be used to link records between surveys and administrative data, or precise geographic coordinates may be needed to compute distances as part of the analysis. Hence, the specific application will restrict the set of applicable data protection systems.</p>
</div>
<div id="suppression" class="section level4">
<h4><span class="header-section-number">0.1.2.2</span> Suppression</h4>
<!-- Also mention rules that point to "suppress regression coefficients when derived from less than x% of sample, or the p-percent rule" as they are applied to "safe output" rules in RDC and MOU scenarios.  -->
<p>Suppression is perhaps the most common form of SDL, and one of the oldest <span class="citation">(Fellegi, <a href="#ref-fellegi_question_1972" role="doc-biblioref">1972</a>)</span>. In their most basic form, suppression rules work as follows:</p>
<ol style="list-style-type: decimal">
<li>Model the sensitivity of a particular data item, table cell, or observation (“disclosure risk”)</li>
<li>Do not allow the release of data items that have excessive disclosure risk (primary suppression)</li>
<li>Do not allow the release of other data from which the sensitive item can be calculated (complementary suppression)</li>
</ol>
<p>Suppression rules can be applied to microdata, in which case the sensitive observations are removed from the microdata, or to tabular data, where the relevant cells are suppressed.</p>
<p>In the case of business microdata, a firm that is unique in its county and industry might be flagged as having high disclosure risk and eliminated from the data. Another less damaging possibility is that just its sensitive attributes are suppressed, so a researcher would still know that there was a firm operating in that industry and location; just not what its other attributes were. For tabular data, the principle is the same. Continuing with the business application, suppose there is one large firm and several smaller competitors in a given industry and location. If the cell is published, it might be possible for its local competitors to learn the receipts of the dominant firm to a high degree of precision.</p>
<p>Cell suppression rules based on this sort of reasoning are called <em>p</em>-percent rules, where <em>p</em> describes the precision with which the largest firm’s information can be learned. A conservative estimate of this occurs when the largest firm’s value is <em>(1-p)%</em> of the cell’s value.<br />
A variant of this rule takes into account prior precision <em>q</em> (the “pq percent rule”). Another rule is known as the <em>n,k</em> rule: a cell is suppressed if <em>n</em> or fewer entities contribute <em>k</em> percent or more of the cell’s value. These rules are frequently applied to statistics produced by national statistical agencies <span class="citation">(Harris-Kojetin et al., <a href="#ref-harris-kojetin_statistical_2005" role="doc-biblioref">2005</a>)</span>. Simpler rules based entirely on cell counts are also encountered, for instance in the Health and Retirement Study <span class="citation">(Health and Retirement Study, <a href="#ref-health_and_retirement_study_disclosure_nodate" role="doc-biblioref">n.d.</a>)</span>. Tables produced using HRS confidential geo-coded data are only allowed to display values when the cell contains three or more records (five for marginal cells).</p>
<p>If a cell in a contingency table is suppressed based on any one of these rules, it could be backed out by using the information in the table margins and the fact that table cells need to sum up to their margins. Some data providers therefore require that additional cells are suppressed to ensure this sort of reverse engineering is not possible. Figuring out how to choose these <em>complementary suppressions</em> in an efficient manner is a non-trivial challenge.</p>
<p>In general, cell suppression is not an ignorable form of SDL. It remains popular because it is easy to explain and does not affect the un-suppressed cells.</p>
<p>Data suppression is clearly non-ignorable, and is quite difficult to correct for suppression in an SDL-aware analysis.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> The features of the data that lead to suppression are often related to the underlying phenomenon of interest. <span class="citation">Chetty &amp; Friedman (<a href="#ref-chetty_practical_2019" role="doc-biblioref">2019</a>)</span> provide a clear illustration. They publish neighborhood-level summaries of intergenerational mobility based on tax records linked to Census data. The underlying microdata are highly sensitive, and to protect privacy they used a variant of a differentially privacy model. They show that if they had instead used a cell suppression rule, the published data would be misleading with respect to the relationship between neighborhood poverty and teen pregnancy because both variables are associated with neighborhood population. Hence, the missingness induced by cell suppression is not ignorable.</p>
<!-- Provide example from CBP -->
<p>Suppression can also be applied to model-based statistics. For instance, after having run a regression, coefficients that correspond to cells with fewer than <em>n</em> cases may be suppressed. This most often occurs when using dichotomous variables (dummy variables), which represent conditional means for particular subgroups.</p>
<blockquote>
<p>In a regression, a researcher includes a set of dummies for interacting occupation and location. When cross-tabulating occupation and location, many cells have less than 5 observations contributing to the coefficient. The data provider requires that these be suppressed.</p>
</blockquote>
<!-- COMMENT Explicitly mention rules that point to "suppress regression coefficients when derived from less than x% of sample, or the p-percent rule" as they are applied to "safe output" rules in RDC and MOU scenarios. -->
<!-- COMMENT In the Sweeney example, the governor's data could have been suppressed, because the observation was flagged as a sensitive observation. -->
<!-- > In the table of X, cell D1 is based on only 1 observation. It would be suppressed as a primary suppression. Cell D2 is at risk of being used to recover cell D1, and thus would be suppressed as a complementary suppression. [need TABLE - harris-kojetin_statistical_2005 has an example]
-->
</div>
<div id="coarsening" class="section level4">
<h4><span class="header-section-number">0.1.2.3</span> Coarsening</h4>
<p>Coarsening takes detailed attributes that can serve as quasi-identifiers and collapses them into a smaller number of categories. Computer scientists call this “generalizing”, and it is also sometimes referred to as “masking”. Coarsening can be applied to quasi-identifiers, to prevent re-identification, or to attributes, to prevent accurate attribute inference. When applied to quasi-identifiers, the concern is that an outsider could use detailed quasi-identifiers to single-out a particular record and learn who it belonged to. By coarsening quasi-identifiers, the set of matching records is increased, raising uncertainty about any re-identified individual’s true identity. In principle, all variables can serve as quasi-identifiers, and the concept of <em>k-anonymity</em> introduced by <span class="citation">Sweeney (<a href="#ref-sweeney_achieving_2002" role="doc-biblioref">2002</a>)</span> is a useful framework for thinking about how to implement coarsening and other microdata SDL. We discuss k-anonymity in the section on <a href="#disclosure-risk">disclosure risk</a>.</p>
<p>Coarsening is common in microdata releases. As a general rule of thumb, it may make sense to consider coarsening variables with heavy tails (earnings, payroll), residuals (truncate range, suppress labels of range). In public-use microdata from the American Community Survey, geographic areas are coarsened until all such areas represent at least 60,000 individuals (REFERENCE). In many data sources, characteristics like age and income, are reported in bins even when the raw data are more detailed. Income topcoding is another common type of coarsening, in which incomes above a certain threshold are replaced with some topcoded value (e.g., $200,000 in the Current Population Survey). When releasing model-based estimates, rounding can satisfy statistical best practice – not releasing numbers beyond their statistical precision – as well as disclosure avoidance principles – by preventing inferences that could be too precise about specific records in the data.</p>
<p>Whether coarsening is ignorable or not depends on the analysis to be performed. Consider the case in which incomes are topcoded above the 95th percentile. This form of SDL is ignorable with respect to estimating the 90th percentile of the income distribution (and all other quantiles below the 95th). However, coarsening age is not ignorable if the goal is to conduct an analysis of behavior of individuals right around some age or date-of-birth cutoff. Coarsening rules should therefore bear in mind the intended analysis for the data and may be usefully paired with restricted-access protocols that allow trusted researchers access to the more detailed data. See <span class="citation">Burkhauser, Feng, Jenkins, &amp; Larrimore (<a href="#ref-burkhauser_estimating_2011" role="doc-biblioref">2011</a>)</span> for an example of the impact of top-coding on estimates of earnings inequality.</p>
</div>
<div id="swapping" class="section level4">
<h4><span class="header-section-number">0.1.2.4</span> Swapping</h4>
<p>The premise behind the technique of <em>swapping</em> is similar to suppression. Again, each record is assigned a level of disclosure risk. Then, any high-risk record is matched to a less risky record on a set of key variables, and all of the other non-key attributes are swapped. The result is a dataset that preserves the distribution among all the key variables used for matching. If the original purpose of the data was to publish cross-tabulations of the matching variables, swapping can produce microdata that are consistent with those tabulations. This approach is more commonly used in censuses and surveys of people or households, and rarely used with establishment data.</p>
<blockquote>
<p>For example, consider our hypothetical health study again, and now suppose we know Ann’s zip code, gender, race, ethnicity, age, smoking behavior, and the size of her household. Ann’s record might be classified as high risk if, say, she has a very large household relative to the rest of the other respondents who are also from her zip code. If the data are used to publish, say, summaries of smoking behavior by age, race, and gender, then Ann’s record would be matched to another record with the same age, race, gender and smoking behavior, and the values of the household size and zip code attributes would be swapped.</p>
</blockquote>
<p>Swapping is ignorable for analyses that only depend on the matching variables, since the relationships among them will be preserved. However, swapping distorts relationships among the other variables, and between the matching variables and the other variables. In the example above, the swapping would be non-ignorable in the context of a study of how smoking behavior various across zip codes. In general, statistical agencies are not willing to publish detailed information about how swapping is implemented since that information could be used to reverse-engineer some of the swaps, undoing the protection. Hence, SDL-aware analysis may not be possible, and inference validity negatively affected.</p>
</div>
<div id="sampling" class="section level4">
<h4><span class="header-section-number">0.1.2.5</span> Sampling</h4>
<p>Sampling is the original SDL technique. Rather than the full confidential microdata, publishing a sample inherently limits the certainty with which an attackers can re-identify records. While sampling can provide a formal privacy guarantee, in modern, detailed surveys, sampling will not in general prevent re-identification. In combination with other tools, like coarsening, sampling may be particularly appealing because, while it is non-ignorable, researchers can adjust their analysis for the sampling using familiar methods. Sampling is often used in conjunction with other methods, including with formally private methods, to amplify the protection provided.</p>
</div>
<div id="noise-infusion" class="section level4">
<h4><span class="header-section-number">0.1.2.6</span> Noise infusion</h4>
<p>Noise infusion can refer to an array of related methods, all of which involve distorting data with randomly distributed noise. There is a key distinction between methods where the microdata are infused with noise (||input noise infusion||), versus methods where noise is added to functions or aggregates of the data before publication (||output noise infusion||).</p>
<p>Noise infusion was developed as a substitute for cell suppression as an approach to protecting tabular summaries of business data. Originally proposed by <span class="citation">Evans, Zayatz, &amp; Slanta (<a href="#ref-evans_using_1998" role="doc-biblioref">1998</a>)</span>, the basic approach assigns each microdata unit (a business establishment) a multiplicative noise factor drawn from a symmetric distribution (e.g., centered on one), and multiplies sensitive (or all) characteristics by that factor. Tabular summaries can then be made from the distorted characteristics. As cell sizes increase, the distortions applied to each unit average out. Thus, while small cells may be quite distorted and thus protected, large cells usually have little distortion. Most cells no longer need to be suppressed. These approaches are used in the U.S. Census Bureau’s Quarterly Workforce Indicators <span class="citation">(Abowd et al., <a href="#ref-abowd_dynamically_2012" role="doc-biblioref">2012</a>, <a href="#ref-abowd_lehd_2009" role="doc-biblioref">2009</a> )</span> and County Business Patterns, with a truncated distribution. When the noise distribution is unbounded, for instance Gaussian, noise infusion may be differentially private, see [chapter on DP].</p>
<p>Noise infusion has the advantage that it mostly eliminates the need to suppress sensitive records or cells, allowing more information to be revealed from the confidential data while maintaining certain confidentiality protections. Noise infusion also generally preserves the means and covariances among variables. However, it will always inflate estimated variances and can lead to bias in estimates of statistical models, and in particular regression coefficients. Hence, noise infusion is, in general, not ignorable. If the details of the noise distribution can be made available to researchers, then it is possible to correct analysis for noise infusion. However, information about the noise distribution can also help an attacker reverse engineer the protections.</p>
</div>
<div id="synthetic-data-and-multiple-imputation" class="section level4">
<h4><span class="header-section-number">0.1.2.7</span> Synthetic data and multiple imputation</h4>
<p>Synthetic data generation and multiple imputation are closely related. In fact, one particular variant of synthetic data as SDL – partially synthetic data – is also known as “suppress and impute” <span class="citation">(Little, <a href="#ref-little_statistical_1993" role="doc-biblioref">1993</a>)</span>. Sensitive values for some or all records are replaced by (multiple) imputations. More generally, fully synthetic data <span class="citation">(Rubin, <a href="#ref-rubin_discussion_1993" role="doc-biblioref">1993</a>)</span> replaces all values with draws from a posterior predictive distribution, estimated given the confidential data. For an overview, see <span class="citation">Raghunathan, Reiter, &amp; Rubin (<a href="#ref-raghunathan_multiple_2003" role="doc-biblioref">2003</a>)</span>, <span class="citation">Little, Liu, &amp; Raghunathan (<a href="#ref-little_statistical_2004" role="doc-biblioref">2004</a>)</span>, and <span class="citation">Drechsler (<a href="#ref-drechsler_synthetic_2011" role="doc-biblioref">2011</a>)</span>.</p>
<p>Synthetic data have been used in the Federal Reserve Board’s Survey of Consumer Finances to protect sensitive income values <span class="citation">(Kennickell, <a href="#ref-kennickell_multiple_1998" role="doc-biblioref">1998</a>)</span>, and in the American Community Survey microdata to protect data from group quarters (such as prisons and university residences; see <span class="citation">Hawala &amp; Rodriguez (<a href="#ref-hawala_disclosure_2009" role="doc-biblioref">2009</a>)</span>). The U.S. Census Bureau’s LODES data, included in the <a href="https://onthemap.ces.census.gov/">OnTheMap</a> application, uses synthetic household data <span class="citation">(Machanavajjhala, Kifer, Abowd, Gehrke, &amp; Vilhuber, <a href="#ref-machanavajjhala_privacy_2008" role="doc-biblioref">2008</a>)</span>. Synthetic data can be used in conjunction with validation servers: researchers use the synthetic data to create complex model-based estimation, then submit their analysis to a remote server with access to the confidential data for validation of the results. Such a mechanism has been used by the U.S. Census Bureau in collaboration with Cornell University for confidential business microdata <span class="citation">(Kinney et al., <a href="#ref-kinney_towards_2011" role="doc-biblioref">2011</a>)</span> and for survey data combined with administrative data <span class="citation">(Abowd, Stinson, &amp; Benedetto, <a href="#ref-abowd_final_2006" role="doc-biblioref">2006</a>)</span>. The term is sometimes used as well for “test” data for remote submission systems, which typically makes no claims as to the validity - it is simply constructed to replicate the data schema of the confidential data, to test statistical code.</p>
</div>
</div>
<div id="metrics" class="section level3">
<h3><span class="header-section-number">0.1.3</span> Metrics</h3>
<!-- How do you measure risk, and the reduction in risk achieved by applying above methods? Mention uniqueness criteria, k-anonymity, l-diversity, matching metrics, etc. -->
<p>The design of an SDL system depends on determinations about what constitutes an acceptable level of disclosure risk, balanced with the proposed uses of the data. There are many different ways to describe and measure disclosure risk. These share in common a sense of how unique a record or combination of attributes is in the data, which corresponds intuitively to a sense of how easy it would be to single-out that record and re-identify the respondent, perhaps aided by a linked dataset. Likewise, there are many different ways to assess whether the released data are suitable, or fit, for their intended use. These quality measures are often based on how closely the released data match the true data on certain statistical summaries, and it will be important for researchers and data custodians to agree on what are the most relevant summaries.</p>
<div id="disclosure-risk" class="section level4">
<h4><span class="header-section-number">0.1.3.1</span> Disclosure Risk</h4>
<p>Early definitions of disclosure risk were based on rules and guidelines derived from institutional knowledge, assessment of summary measures, and reidentification experiments (<span class="citation">Harris-Kojetin et al. (<a href="#ref-harris-kojetin_statistical_2005" role="doc-biblioref">2005</a>)</span>). Statisticians have subsequently developed more formal models to measure risk of re-identification for specific types of publication and with particular threat models. For instance, <span class="citation">Shlomo &amp; Skinner (<a href="#ref-shlomo_assessing_2010" role="doc-biblioref">2010</a>)</span> model reidentification risk in survey microdata when an attacker is matching on certain categorical variables.</p>
<p>Recently, computer scientists and statisticians have introduced more general concepts of disclosure risk and data privacy. Latanya Sweeny proposed the concept of <span class="math inline">\(k\)</span>-anonymity <span class="citation">(Sweeney, <a href="#ref-sweeney_achieving_2002" role="doc-biblioref">2002</a>)</span> which defines disclosure risk in terms of the number of records that share the same combination of attributes. If a single record is uniquely identified by some combination of attributes, disclosure risk is high. Sweeny says that a dataset can be called <span class="math inline">\(k\)</span>-anonymous if for all feasible combinations of attributes, at least <span class="math inline">\(k\)</span> records have that combination. Intuitively, increases in <span class="math inline">\(k\)</span> reduce the risk that observations can be singled out by linking other datasets that contain the same attributes. The concept of <span class="math inline">\(k\)</span>-anonymity can provide some guidance when thinking about how to implement the SDL systems described above. For example, if records are uniquely identified by age, race, and gender, then one might collapse age into brackets until there are at least <span class="math inline">\(k&gt;1\)</span> records for each such combination.</p>
<p>However, <span class="math inline">\(k\)</span>-anonymity does not protect against attribute disclosure. If all <span class="math inline">\(k\)</span> observations with the same combination of attributes also share the same sensitive attribute, say smoking behavior, then the published data do not fully prevent disclosure of smoking behavior. Recognizing this, <span class="citation">Machanavajjhala, Kifer, Gehrke, &amp; Venkitasubramaniam (<a href="#ref-machanavajjhala_l-diversity_2007" role="doc-biblioref">2007</a>)</span> introduce the concept of <span class="math inline">\(\ell\)</span>-diversity. The idea is that whenever a group of records are identical on some set of variables, there must be a certain amount of heterogeneity in important sensitive traits. If a certain group of records match on a set of quasi-identifiers and also all share the same smoking status, then to achieve <span class="math inline">\(\ell\)</span>-diversity, we might alter the reported smoking behavior of some fraction (<span class="math inline">\(\ell\)</span>) of the records - a form of noise infusion.</p>
</div>
<div id="data-quality" class="section level4">
<h4><span class="header-section-number">0.1.3.2</span> Data Quality</h4>
<p>When the released data or output is tabular (histograms, cross-tabulations) or is a limited set of population or model parameters (means, coefficients), a set of distance-based metrics (so-called “<span class="math inline">\(\ell_p\)</span> distance” metrics) can be used to compare the quality of the perturbed data. Note that this is a specific metric, as it is limited to those statistics taken into account - the data quality may be very poor in non-measured attributes! For <span class="math inline">\(p=1\)</span>, the <span class="math inline">\(\ell_1\)</span> distance is the sum of absolute differences between the confidential and perturbed data. For <span class="math inline">\(p = 2\)</span>, the <span class="math inline">\(l_2\)</span> distance is the sum of squared differences between the two datasets (normalized by <span class="math inline">\(n\)</span> the number of observations, it is the Mean Squared Error, MSE). In settings where it is important to measure data quality over an entire distribution, the Kullbach-Leibler (KL) divergence measure can also be used. The KL-divergence is related to the concept of entropy from information theory and, loosely, measures the amount of surprise associated with seeing an observation drawn from one distribution when you expected them to come from another distribution. Other metrics are based on propensity scores <span class="citation">(Snoke, Raab, Nowok, Dibben, &amp; Slavkovic, <a href="#ref-snoke_general_2018" role="doc-biblioref">2018</a>; Woo, Reiter, Oganian, &amp; Karr, <a href="#ref-woo_global_2009" role="doc-biblioref">2009</a>)</span>. More specific measures will often compare specific analysis output, a task that is quite difficult to conduct in general. <span class="citation">Reiter, Oganian, &amp; Karr (<a href="#ref-reiter_verification_2009" role="doc-biblioref">2009</a>)</span> propose to summarize the difference between regression coefficients when analyses can be run on both confidential and protected data,in the context of verification servers.</p>
</div>
</div>
<div id="tools" class="section level3">
<h3><span class="header-section-number">0.1.4</span> Tools</h3>
<p>Multiple institutions provide guidance to researchers who wish to apply SDL techniques. We point in particular to a useful checklist by ICPSR <span class="citation">(ICPSR, <a href="#ref-icpsr_disclosure_2020" role="doc-biblioref">2020</a>)</span> for useful practical guidance.</p>
<p>Because the particular requirements of a given SDL system are often unique, they are frequently implemented using custom programming. Nevertheless, a few tools are of more general applicability. The listing below is almost certainly incomplete, but should provide practitioners with a starting point in applying SDL for data publication.</p>
<p>Statistics Netherlands maintains the ARGUS software for SDL <span class="citation">(Hundepool &amp; Willenborg, <a href="#ref-hundepool_argus_1998" role="doc-biblioref">1998</a>)</span>, including τ-ARGUS to protect tabular data <span class="citation">(De Wolf, <a href="#ref-de_wolf_-argus_2018" role="doc-biblioref">2018</a>)</span>, and μ-ARGUS for protecting microdata <span class="citation">(Hundepool &amp; Ramaswamy, <a href="#ref-hundepool_-argus_2018" role="doc-biblioref">2018</a>)</span>. The software appears to be widely used in statistical agencies in Europe. An open-source R, <code>sdcMicro</code> implements a full suite of tools needed to apply SDL, from computation of risk measures, including <span class="math inline">\(k\)</span>-anonymity and <span class="math inline">\(l\)</span>-diversity, to implementation of SDL methods and the computation of data quality measures <span class="citation">(Templ, Kowarik, &amp; Meindl, <a href="#ref-templ_statistical_2015" role="doc-biblioref">2015</a> ; Templ, Meindl, &amp; Kowarik, <a href="#ref-templ_sdcmicro_2020" role="doc-biblioref">2020</a>)</span>.</p>
<p>Simpler tools, focusing on removing direct identifiers, can be found at J-PAL (for Stata (<a href="https://github.com/J-PAL/stata_PII_scan">stata_PII_scan</a>) <span class="citation">(J-PAL, <a href="#ref-j-pal_j-palstata_pii_scan_2020" role="doc-biblioref">2020</a><a href="#ref-j-pal_j-palstata_pii_scan_2020" role="doc-biblioref">b</a>)</span> and R (<a href="https://github.com/J-PAL/PII-Scan">PII-scan</a> <span class="citation">(J-PAL, <a href="#ref-j-pal_j-palpii-scan_2020" role="doc-biblioref">2020</a><a href="#ref-j-pal_j-palpii-scan_2020" role="doc-biblioref">a</a>)</span>) and Innovations for Poverty Action (for Python or Windows <a href="https://github.com/PovertyAction/PII_detection">PII_detection</a> <span class="citation">(Innovations for Poverty Action, <a href="#ref-innovations_for_poverty_action_povertyactionpii_detection_2020" role="doc-biblioref">2020</a>)</span>).</p>
<p>A number of R packages facilitate generation of synthetic data. <span class="citation">Raab, Nowok, &amp; Dibben (<a href="#ref-raab_practical_2016" role="doc-biblioref">2016</a>)</span> and <span class="citation">Nowok, Raab, &amp; Dibben (<a href="#ref-nowok_synthpop_2016" role="doc-biblioref">2016</a>)</span> provide a flexible and up-to-date packages with methods for generating synthetic microdata. <span class="citation">Templ et al. (<a href="#ref-templ_simpop_2019" role="doc-biblioref">2019</a>)</span> can also generate synthetic populations from aggregate data, which can be useful for testing SDL systems on non-sensitive data. In some cases, one might also consider using general-purpose software for multiple imputation for data synthesis.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
</div>
<div id="conclusion" class="section level3">
<h3><span class="header-section-number">0.1.5</span> Conclusion</h3>
<p>Disclosure avoidance attempts to strike a balance between usability of the data (in a particular environment and context) and the ability of others to infer identity and attributes of the people and institutions represented in the data. This chapter provided an overview of the various techniques traditionally used to modify the data to achieve that goal. The techniques range from the simple to complex, with varying degrees of confidentiality protection.</p>
</div>
<div id="about-the-authors" class="section level3">
<h3><span class="header-section-number">0.1.6</span> About the Authors</h3>
<p>Ian M. Schmutte is Associate Professor in the Department of Economics at the University of Georgia. Schmutte is currently working with the U.S. Census Bureau on new methods for protecting confidential data. His research has appeared in the American Economic Review, Journal of Labor Economics, Journal of Human Resources, Journal of Business and Economic Statistics, and the Brookings Papers on Economic Activity.</p>
<p>Lars Vilhuber is the Executive Director of the <a href="http://www.ilr.cornell.edu/ldi" title="LDI">Labor Dynamics Institute</a> at <a href="http://www.cornell.edu" title="Cornell University">Cornell University</a>. He has worked for many years with the <a href="https://www.census.gov/research/">Research and Methodology Directorate</a> at the U.S. Census Bureau on a variety of projects, including implementing disclosure avoidance techniques. He is a member of governing or scientific committees of secure data access center in Canada (<a href="https://crdcn.org/">CRDCN</a> and France (<a href="http://casd.eu">CASD</a>, and a member of the <a href="https://www.amstat.org/">American Statistical Association</a>‘s <a href="https://community.amstat.org/cpc/home">Committee on Privacy and Confidentiality</a>. He is the inaugural Data Editor for the <a href="https://www.aeaweb.org/">American Economic Association</a>, and the Managing (Executive) Editor of the <a href="https://journalprivacyconfidentiality.org/">Journal of Privacy and Confidentiality</a>. He is the <a href="https://www.povertyactionlab.org/initiative/innovations-data-experiments-action">Co-Chair, Innovations in Data and Experiments for Action (IDEA) Initiative</a> at <a href="https://www.povertyactionlab.org/">J-PAL</a>.</p>
</div>
<div id="acknowledgements" class="section level3">
<h3><span class="header-section-number">0.1.7</span> Acknowledgements</h3>
<p>This chapter draws on <span class="citation">Abowd et al. (<a href="#ref-abowd_introductory_2019" role="doc-biblioref">2019</a>)</span> and the INFO7470 class at Cornell University <span class="citation">Abowd &amp; Vilhuber (<a href="#ref-abowd_session_2016" role="doc-biblioref">2016</a>)</span> . We gratefully acknowledge the support of Alfred P. <a href="https://sloan.org/grant-detail/6845">Sloan Foundation Grant G-2015-13903</a> and <a href="http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=1131848">NSF Grant SES-1131848</a> for the earlier work.</p>
</div>
<div id="disclaimer" class="section level3">
<h3><span class="header-section-number">0.1.8</span> Disclaimer</h3>
<p>The views expressed in this paper are those of the authors and not those of the U.S. Census Bureau or other sponsors.</p>
</div>
<div id="references" class="section level3">
<h3><span class="header-section-number">0.1.9</span> References</h3>
<!-- # References -->
<!-- If you need PDF output, uncomment bookdown::pdf_book above in YAML. You will need a LaTeX installation, e.g., https://yihui.name/tinytex/ -->
<!--chapter:end:index.Rmd-->
<div id="refs" class="references">
<div id="ref-abowd_dynamically_2012">
<p>Abowd, J. M., Gittings, R. K. K., McKinney, K. L., Stephens, B., Vilhuber, L., &amp; Woodcock, S. D. (2012). <em>Dynamically Consistent Noise Infusion and Partially Synthetic Data as Confidentiality Protection Measures for Related Time Series</em> (No. 12-13). <a href="https://doi.org/10.2139/ssrn.2159800">https://doi.org/10.2139/ssrn.2159800</a></p>
</div>
<div id="ref-abowd_introductory_2019">
<p>Abowd, J. M., Schmutte, I., Sexton, W., &amp; Vilhuber, L. (2019). <em>Introductory Readings in Formal Privacy for Economists</em> (Document No. 2662639). <a href="https://doi.org/10.5281/zenodo.2662639">https://doi.org/10.5281/zenodo.2662639</a></p>
</div>
<div id="ref-abowd_lehd_2009">
<p>Abowd, J. M., Stephens, B. E., Vilhuber, L., Andersson, F., McKinney, K. L., Roemer, M., &amp; Woodcock, S. D. (2009). The LEHD Infrastructure Files and the Creation of the Quarterly Workforce Indicators. In T. Dunne and J B Jensen and M J Roberts (Ed.), <em>Producer Dynamics: New Evidence from Micro Data</em>. University of Chicago Press.</p>
</div>
<div id="ref-abowd_final_2006">
<p>Abowd, J. M., Stinson, M., &amp; Benedetto, G. (2006). <em>Final Report to the Social Security Administration on the SIPP/SSA/IRS Public Use File Project</em> (No. 1813/43929). Retrieved from U.S. Census Bureau website: <a href="http://hdl.handle.net/1813/43929">http://hdl.handle.net/1813/43929</a></p>
</div>
<div id="ref-abowd_session_2016">
<p>Abowd, J. M., &amp; Vilhuber, L. (2016). <em>Session 12: Statistical Tools: Methods of Confidentiality Protection</em> (Presentation No. 45060). Retrieved from Labor Dynamics Institute, Cornell University website: <a href="https://hdl.handle.net/1813/45060">https://hdl.handle.net/1813/45060</a></p>
</div>
<div id="ref-abowd_economic_2019">
<p>Abowd, J., &amp; Schmutte, I. (2019). An Economic Analysis of Privacy Protection and Statistical Accuracy as Social Choices. <em>American Economic Review</em>, <em>109</em>(1), 171–202. <a href="https://doi.org/10.1257/aer.20170627">https://doi.org/10.1257/aer.20170627</a></p>
</div>
<div id="ref-abowd_economic_2015">
<p>Abowd, J., &amp; Schmutte, I. M. (2015). Economic analysis and statistical disclosure limitation. <em>Brookings Papers on Economic Activity</em>, 221–267. <a href="https://doi.org/10.1353/eca.2016.0004">https://doi.org/10.1353/eca.2016.0004</a></p>
</div>
<div id="ref-acquisti_economics_2016">
<p>Acquisti, A., Taylor, C., &amp; Wagman, L. (2016). The Economics of Privacy. <em>Journal of Economic Literature</em>, <em>54</em>(2), 442–492. <a href="https://doi.org/10.1257/jel.54.2.442">https://doi.org/10.1257/jel.54.2.442</a></p>
</div>
<div id="ref-auxier_americans_2019">
<p>Auxier, B., Rainie, L., Anderson, M., Perrin, A., Kumar, M., &amp; Turner, E. (2019). <em>Americans and Privacy: Concerned, Confused and Feeling Lack of Control Over Their Personal Information</em>. Retrieved from Pew Research Center website: <a href="https://www.pewresearch.org/internet/2019/11/15/americans-and-privacy-concerned-confused-and-feeling-lack-of-control-over-their-personal-information/">https://www.pewresearch.org/internet/2019/11/15/americans-and-privacy-concerned-confused-and-feeling-lack-of-control-over-their-personal-information/</a></p>
</div>
<div id="ref-black_6_2020">
<p>Black, K., Ramos, G. A., &amp; Biscardi, G. (2020). 6 Months Until Brazil’s LGPD Takes Effect – Are You Ready? Retrieved from <a href="https://www.natlawreview.com/article/6-months-until-brazil-s-lgpd-takes-effect-are-you-ready">https://www.natlawreview.com/article/6-months-until-brazil-s-lgpd-takes-effect-are-you-ready</a></p>
</div>
<div id="ref-burkhauser_estimating_2011">
<p>Burkhauser, R. V., Feng, S., Jenkins, S. P., &amp; Larrimore, J. (2011). Estimating trends in US income inequality using the Current Population Survey: The importance of controlling for censoring. <em>The Journal of Economic Inequality</em>, <em>9</em>(3), 393–415. <a href="https://doi.org/10.1007/s10888-010-9131-6">https://doi.org/10.1007/s10888-010-9131-6</a></p>
</div>
<div id="ref-buuren_mice_2011">
<p>Buuren, S. van, &amp; Groothuis-Oudshoorn, K. (2011). Mice: Multivariate Imputation by Chained Equations in R. <em>Journal of Statistical Software</em>, <em>45</em>(3), 1–67. Retrieved from <a href="https://www.jstatsoft.org/v45/i03/">https://www.jstatsoft.org/v45/i03/</a></p>
</div>
<div id="ref-cameron_practitioners_2015">
<p>Cameron, A. C., &amp; Miller, D. L. (2015). A Practitioner’s Guide to Cluster-Robust Inference. <em>Journal of Human Resources</em>, <em>50</em>(2), 317–372. <a href="https://doi.org/10.3368/jhr.50.2.317">https://doi.org/10.3368/jhr.50.2.317</a></p>
</div>
<div id="ref-chetty_practical_2019">
<p>Chetty, R., &amp; Friedman, J. N. (2019). A Practical Method to Reduce Privacy Loss When Disclosing Statistics Based on Small Samples. <em>Journal of Privacy and Confidentiality</em>, <em>9</em>(2). <a href="https://doi.org/10.29012/jpc.716">https://doi.org/10.29012/jpc.716</a></p>
</div>
<div id="ref-cohen_towards_2020">
<p>Cohen, A., &amp; Nissim, K. (2020). Towards formalizing the GDPR’s notion of singling out. <em>Proceedings of the National Academy of Sciences of the United States of America</em>, <em>117</em>(15), 8344–8352. <a href="https://doi.org/10.1073/pnas.1914598117">https://doi.org/10.1073/pnas.1914598117</a></p>
</div>
<div id="ref-couper_risk_2008">
<p>Couper, M. P., Singer, E., Conrad, F. G., &amp; Groves, R. M. (2008). Risk of disclosure, perceptions of risk, and concerns about privacy and confidentiality as factors in survey participation. <em>Journal of Official Statistics</em>, <em>24</em>(2), 255. Retrieved from <a href="http://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/risk-of-disclosure-perceptions-of-risk-and-concerns-about-privacy-and-confidentiality-as-factors-in-survey-participation.pdf">http://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/risk-of-disclosure-perceptions-of-risk-and-concerns-about-privacy-and-confidentiality-as-factors-in-survey-participation.pdf</a></p>
</div>
<div id="ref-cummings_adaptive_2016">
<p>Cummings, R., Ligett, K., Nissim, K., Roth, A., &amp; Wu, Z. S. (2016). Adaptive Learning with Robust Generalization Guarantees. <em>CoRR</em>, <em>abs/1602.07726</em>. Retrieved from <a href="http://arxiv.org/abs/1602.07726">http://arxiv.org/abs/1602.07726</a></p>
</div>
<div id="ref-dalenius_towards_1977">
<p>Dalenius, T. (1977). Towards a methodology for statistical disclosure control. <em>Statistik Tidskrift</em>, <em>15</em>, 429–444. <a href="https://doi.org/10.1145/320613.320616">https://doi.org/10.1145/320613.320616</a></p>
</div>
<div id="ref-department_of_health_and_human_services_methods_2012">
<p>Department of Health and Human Services. (2012). Methods for De-identification of PHI [Text]. Retrieved from <a href="https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html">https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html</a></p>
</div>
<div id="ref-de_wolf_-argus_2018">
<p>De Wolf, P.-P. (2018). <em>Τ-ARGUS</em>. Retrieved from <a href="http://research.cbs.nl/casc/tau.htm">http://research.cbs.nl/casc/tau.htm</a></p>
</div>
<div id="ref-drechsler_synthetic_2011">
<p>Drechsler, J. (2011). <em>Synthetic Datasets for Statistical Disclosure Control: Theory and Implementation</em>. <a href="https://doi.org/10.1007/978-1-4614-0326-5">https://doi.org/10.1007/978-1-4614-0326-5</a></p>
</div>
<div id="ref-duncan_disclosure-limited_1986">
<p>Duncan, G., &amp; Lambert, D. (1986). Disclosure-limited data dissemination. <em>Journal of the American Statistical Association</em>, <em>81</em>(393), 10–18. <a href="https://doi.org/10.1080/01621459.1986.10478229">https://doi.org/10.1080/01621459.1986.10478229</a></p>
</div>
<div id="ref-duncan_statistical_2011">
<p>Duncan, G. T., Elliot, M., &amp; Salazar-González, J.-J. (2011). <em>Statistical confidentiality: Principles and practice</em>. <a href="https://doi.org/10.1111/j.1751-5823.2012.00196_11.x">https://doi.org/10.1111/j.1751-5823.2012.00196_11.x</a></p>
</div>
<div id="ref-duncan_private_1993">
<p>Duncan, G. T., Jabine, T. B., &amp; Wolf, V. A. de (Eds.). (1993). <em>Private Lives and Public Policies: Confidentiality and Accessibility of Government Statistics</em>. <a href="https://doi.org/10.17226/2122">https://doi.org/10.17226/2122</a></p>
</div>
<div id="ref-dupriez_dissemination_2010">
<p>Dupriez, O., &amp; Boyko, E. (2010). <em>Dissemination of Microdata Files - Principles, Procedures and Practices</em> (Working Paper No. 005). Retrieved from The World Bank website: <a href="http://ihsn.org/dissemination-of-microdata-files">http://ihsn.org/dissemination-of-microdata-files</a></p>
</div>
<div id="ref-dwork_generalization_2015">
<p>Dwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., &amp; Roth, A. (2015). Generalization in Adaptive Data Analysis and Holdout Reuse. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, &amp; R. Garnett (Eds.), <em>Advances in Neural Information Processing Systems 28</em> (pp. 2341–2349). Retrieved from <a href="http://papers.nips.cc/paper/5993-generalization-in-adaptive-data-analysis-and-holdout-reuse.pdf">http://papers.nips.cc/paper/5993-generalization-in-adaptive-data-analysis-and-holdout-reuse.pdf</a></p>
</div>
<div id="ref-dwork_fienberg_2018">
<p>Dwork, C., &amp; Ullman, J. (2018). The Fienberg Problem: How to Allow Human Interactive Data Analysis in the Age of Differential Privacy. <em>Journal of Privacy and Confidentiality</em>, <em>8</em>(1). <a href="https://doi.org/10.29012/jpc.687">https://doi.org/10.29012/jpc.687</a></p>
</div>
<div id="ref-evans_using_1998">
<p>Evans, T., Zayatz, L., &amp; Slanta, J. (1998). Using Noise for Disclosure Limitation of Establishment Tabular Data. <em>Journal of Official Statistics</em>, <em>14</em>(4), 537–551.</p>
</div>
<div id="ref-fellegi_question_1972">
<p>Fellegi, I. P. (1972). On the Question of Statistical Confidentiality. <em>Journal of the American Statistical Association</em>, <em>67</em>(337), 7–18. <a href="https://doi.org/10.2307/2284695">https://doi.org/10.2307/2284695</a></p>
</div>
<div id="ref-garfinkel_-identification_2015">
<p>Garfinkel, S. (2015). <em>De-Identification of Personal Information</em> (Internal Report No. 8053). <a href="https://doi.org/10.6028/nist.ir.8053">https://doi.org/10.6028/nist.ir.8053</a></p>
</div>
<div id="ref-goroff_balancing_2015">
<p>Goroff, D. L. (2015). Balancing privacy versus accuracy in research protocols. <em>Science</em>, <em>347</em>(6221), 479–480. <a href="https://doi.org/10.1126/science.aaa3483">https://doi.org/10.1126/science.aaa3483</a></p>
</div>
<div id="ref-harris-kojetin_statistical_2005">
<p>Harris-Kojetin, B. A., Alvey, W. L., Carlson, L., Cohen, S. B., Cohen, S. H., Cox, L. H., … Groves, R. (2005). <em>Statistical Policy Working Paper 22: Report on Statistical Disclosure Limitation Methodology</em> [Research Report]. Retrieved from U.S. Federal Committee on Statistical Methodology website: <a href="https://nces.ed.gov/FCSM/pdf/spwp22.pdf">https://nces.ed.gov/FCSM/pdf/spwp22.pdf</a></p>
</div>
<div id="ref-hawala_disclosure_2009">
<p>Hawala, S., &amp; Rodriguez, R. (2009). <em>Disclosure avoidance for group quarters in the American Community Survey: Details of the synthetic data method</em> [Presentation]. Retrieved from <a href="https://ecommons.cornell.edu/handle/1813/47676">https://ecommons.cornell.edu/handle/1813/47676</a></p>
</div>
<div id="ref-health_and_retirement_study_disclosure_nodate">
<p>Health and Retirement Study. (n.d.). <em>Disclosure Limitation Review</em>. Retrieved from <a href="https://hrs.isr.umich.edu/data-products/restricted-data/disclosure-limitation-review">https://hrs.isr.umich.edu/data-products/restricted-data/disclosure-limitation-review</a></p>
</div>
<div id="ref-hirshleifer_privacy_1980">
<p>Hirshleifer, J. (1980). Privacy: Its origin, function, and future. <em>The Journal of Legal Studies</em>, <em>9</em>(4), 649–664. <a href="https://doi.org/10.1086/467659">https://doi.org/10.1086/467659</a></p>
</div>
<div id="ref-hsu_differential_2014">
<p>Hsu, J., Gaboardi, M., Haeberlen, A., Khanna, S., Narayan, A., Pierce, B. C., &amp; Roth, A. (2014). Differential Privacy: An Economic Method for Choosing Epsilon. <em>2014 IEEE 27th Computer Security Foundations Symposium</em>, 398–410. <a href="https://doi.org/10.1109/CSF.2014.35">https://doi.org/10.1109/CSF.2014.35</a></p>
</div>
<div id="ref-hundepool_-argus_2018">
<p>Hundepool, A., &amp; Ramaswamy, R. (2018). <em>Μ-ARGUS</em>. Retrieved from <a href="http://research.cbs.nl/casc/mu.htm">http://research.cbs.nl/casc/mu.htm</a></p>
</div>
<div id="ref-hundepool_argus_1998">
<p>Hundepool, A., &amp; Willenborg, L. (1998). ARGUS, Software Packages for Statistical Disclosure Control. In R. Payne &amp; P. Green (Eds.), <em>COMPSTAT</em> (pp. 341–345). <a href="https://doi.org/10.1007/978-3-662-01131-7_45">https://doi.org/10.1007/978-3-662-01131-7_45</a></p>
</div>
<div id="ref-icpsr_disclosure_2020">
<p>ICPSR. (2020). <em>Disclosure Risk Worksheet</em> (Document No. 156095). Retrieved from University of Michigan website: <a href="http://hdl.handle.net/2027.42/156095">http://hdl.handle.net/2027.42/156095</a></p>
</div>
<div id="ref-innovations_for_poverty_action_povertyactionpii_detection_2020">
<p>Innovations for Poverty Action. (2020). <em>PovertyAction/PII_detection</em>. Retrieved from <a href="https://github.com/PovertyAction/PII_detection">https://github.com/PovertyAction/PII_detection</a></p>
</div>
<div id="ref-j-pal_j-palpii-scan_2020">
<p>J-PAL. (2020a). <em>J-PAL/PII-Scan</em>. Retrieved from <a href="https://github.com/J-PAL/PII-Scan">https://github.com/J-PAL/PII-Scan</a></p>
</div>
<div id="ref-j-pal_j-palstata_pii_scan_2020">
<p>J-PAL. (2020b). <em>J-PAL/stata_PII_scan</em>. Retrieved from <a href="https://github.com/J-PAL/stata_PII_scan">https://github.com/J-PAL/stata_PII_scan</a></p>
</div>
<div id="ref-kennickell_multiple_1998">
<p>Kennickell, A. B. (1998). Multiple imputation in the Survey of Consumer Finances. <em>Proceedings of the Section on Survey Research</em>. Retrieved from <a href="https://www.federalreserve.%20gov/econresdata/scf/files/impute98.pdf">https://www.federalreserve. gov/econresdata/scf/files/impute98.pdf</a></p>
</div>
<div id="ref-kinney_towards_2011">
<p>Kinney, S. K., Reiter, J. P., Reznek, A. P., Miranda, J., Jarmin, R. S., &amp; Abowd, J. M. (2011). Towards Unrestricted Public Use Business Microdata: The Synthetic Longitudinal Business Database. <em>International Statistical Review</em>, <em>79</em>(3), 362–384. <a href="https://doi.org/10.1111/j.1751-5823.2011.00153.x">https://doi.org/10.1111/j.1751-5823.2011.00153.x</a></p>
</div>
<div id="ref-kopper_j-pal_2020">
<p>Kopper, S., Sautmann, A., &amp; Turitto, J. (2020). <em>J-PAL Guide to de-identifying data</em> (p. 12). Retrieved from J-PAL Global website: <a href="https://www.povertyactionlab.org/sites/default/files/research-resources/J-PAL-guide-to-deidentifying-data.pdf">https://www.povertyactionlab.org/sites/default/files/research-resources/J-PAL-guide-to-deidentifying-data.pdf</a></p>
</div>
<div id="ref-little_statistical_1993">
<p>Little, R. J. (1993). Statistical analysis of masked data. <em>Journal of Official Statistics</em>, <em>9</em>(2), 407–426. Retrieved from <a href="http://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/statistical-analysis-of-masked-data.pdf">http://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/statistical-analysis-of-masked-data.pdf</a></p>
</div>
<div id="ref-little_statistical_2004">
<p>Little, R. J. A., Liu, F., &amp; Raghunathan, T. E. (2004). Statistical disclosure techniques based on multiple imputation. In A. Gelman &amp; X. L. Meng (Eds.), <em>Applied Bayesian modeling and causal inference from incomplete-data perspectives</em> (pp. 141–152). Retrieved from <a href="https://doi.org/10.1002/0470090456.ch13">https://doi.org/10.1002/0470090456.ch13</a></p>
</div>
<div id="ref-liu_statistical_2020">
<p>Liu, F. (2020). A Statistical Overview on Data Privacy. <em>arXiv:2007.00765 [Cs, Stat]</em>. Retrieved from <a href="http://arxiv.org/abs/2007.00765">http://arxiv.org/abs/2007.00765</a></p>
</div>
<div id="ref-machanavajjhala_privacy_2008">
<p>Machanavajjhala, A., Kifer, D., Abowd, J., Gehrke, J., &amp; Vilhuber, L. (2008). Privacy: Theory meets practice on the map. <em>Proceedings of the 2008 IEEE 24th International Conference on Data Engineering</em>, 277–286. <a href="https://doi.org/10.1109/ICDE.2008.4497436">https://doi.org/10.1109/ICDE.2008.4497436</a></p>
</div>
<div id="ref-machanavajjhala_l-diversity_2007">
<p>Machanavajjhala, A., Kifer, D., Gehrke, J., &amp; Venkitasubramaniam, M. (2007). L-diversity: Privacy beyond k-anonymity. <em>ACM Transactions on Knowledge Discovery from Data</em>, <em>1</em>(1). <a href="https://doi.org/10.1145/1217299.1217302">https://doi.org/10.1145/1217299.1217302</a></p>
</div>
<div id="ref-marini_comparing_2018">
<p>Marini, A., Kateifides, A., &amp; Bates, J. (2018). <em>Comparing privacy laws: GDPR v. CCPA</em>. Retrieved from Future of Privacy Forum website: <a href="https://fpf.org/wp-content/uploads/2018/11/GDPR_CCPA_Comparison-Guide.pdf">https://fpf.org/wp-content/uploads/2018/11/GDPR_CCPA_Comparison-Guide.pdf</a></p>
</div>
<div id="ref-moulton_random_1986">
<p>Moulton, B. R. (1986). Random group effects and the precision of regression estimates. <em>Journal of Econometrics</em>, <em>32</em>(3), 385–397. <a href="https://doi.org/10.1016/0304-4076(86)90021-7">https://doi.org/10.1016/0304-4076(86)90021-7</a></p>
</div>
<div id="ref-nowok_synthpop_2016">
<p>Nowok, B., Raab, G. M., &amp; Dibben, C. (2016). Synthpop: Bespoke Creation of Synthetic Data in R. <em>Journal of Statistical Software</em>, <em>74</em>(1), 1–26. <a href="https://doi.org/10.18637/jss.v074.i11">https://doi.org/10.18637/jss.v074.i11</a></p>
</div>
<div id="ref-panakal_indias_2019">
<p>Panakal, D. D. (2019). India’s Proposed Privacy Law Allows Government Access and Some Data Localization. Retrieved from <a href="https://www.natlawreview.com/article/india-s-proposed-privacy-law-allows-government-access-and-some-data-localization">https://www.natlawreview.com/article/india-s-proposed-privacy-law-allows-government-access-and-some-data-localization</a></p>
</div>
<div id="ref-posner_economics_1981">
<p>Posner, R. A. (1981). The Economics of Privacy. <em>The American Economic Review</em>, <em>71</em>(2), 405–409. Retrieved from <a href="https://www.jstor.org/stable/1815754">https://www.jstor.org/stable/1815754</a></p>
</div>
<div id="ref-raab_practical_2016">
<p>Raab, G. M., Nowok, B., &amp; Dibben, C. (2016). Practical Data Synthesis for Large Samples. <em>Journal of Privacy and Confidentiality</em>, <em>7</em>(3), 67–97. <a href="https://doi.org/10.29012/jpc.v7i3.407">https://doi.org/10.29012/jpc.v7i3.407</a></p>
</div>
<div id="ref-raghunathan_multiple_2003">
<p>Raghunathan, T. E., Reiter, J. P., &amp; Rubin, D. B. (2003). Multiple Imputation for Statistical Disclosure Limitation. <em>Journal of Official Statistics</em>, <em>19</em>(1). Retrieved from <a href="http://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/multiple-imputation-for-statistical-disclosure-limitation.pdf">http://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/multiple-imputation-for-statistical-disclosure-limitation.pdf</a></p>
</div>
<div id="ref-reiter_verification_2009">
<p>Reiter, J. P., Oganian, A., &amp; Karr, A. F. (2009). Verification servers: Enabling analysts to assess the quality of inferences from public use data. <em>Computational Statistics &amp; Data Analysis</em>, <em>53</em>(4), 1475–1482. <a href="https://doi.org/10.1016/j.csda.2008.10.006">https://doi.org/10.1016/j.csda.2008.10.006</a></p>
</div>
<div id="ref-rubin_discussion_1993">
<p>Rubin, D. B. (1993). Discussion: Statistical disclosure limitation. <em>Journal of Official Statistics</em>, <em>9</em>(2), 461–468. Retrieved from <a href="http://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/discussion-statistical-disclosure-limitation2.pdf">http://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/discussion-statistical-disclosure-limitation2.pdf</a></p>
</div>
<div id="ref-shlomo_assessing_2010">
<p>Shlomo, N., &amp; Skinner, C. (2010). Assessing the protection provided by misclassification-based disclosure limitation methods for survey microdata. <em>Annals of Applied Statistics</em>, <em>4</em>(3), 1291–1310. <a href="https://doi.org/10.1214/09-AOAS317">https://doi.org/10.1214/09-AOAS317</a></p>
</div>
<div id="ref-snoke_general_2018">
<p>Snoke, J., Raab, G. M., Nowok, B., Dibben, C., &amp; Slavkovic, A. (2018). General and specific utility measures for synthetic data. <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em>, <em>181</em>(3), 663–688. <a href="https://doi.org/10.1111/rssa.12358">https://doi.org/10.1111/rssa.12358</a></p>
</div>
<div id="ref-spencer_effects_2015">
<p>Spencer, B. D., &amp; Seeskin, Z. H. (2015). Effects of Census Accuracy on Apportionment of Congress and Allocations of Federal Funds. <em>JSM Proceedings, Government Statistics Section</em>, 3061–3075. Retrieved from <a href="https://www.ipr.northwestern.edu/publications/papers/2015/ipr-wp-15-05.html">https://www.ipr.northwestern.edu/publications/papers/2015/ipr-wp-15-05.html</a></p>
</div>
<div id="ref-stigler_introduction_1980">
<p>Stigler, G. J. (1980). An Introduction to Privacy in Economics and Politics. <em>Journal of Legal Studies</em>, <em>9</em>(4), 623–644. <a href="https://doi.org/10.2307/724174">https://doi.org/10.2307/724174</a></p>
</div>
<div id="ref-sweeney_achieving_2002">
<p>Sweeney, L. (2002). Achieving k-anonymity privacy protection using generalization and suppression. <em>International Journal on Uncertainty, Fuzziness and Knowledge-Based Systems</em>, <em>10</em>(5), 571–588. <a href="https://doi.org/10.1142/s021848850200165x">https://doi.org/10.1142/s021848850200165x</a></p>
</div>
<div id="ref-templ_statistical_2015">
<p>Templ, M., Kowarik, A., &amp; Meindl, B. (2015). Statistical Disclosure Control for Micro-Data Using the R Package sdcMicro. <em>Journal of Statistical Software</em>, <em>67</em>(4). <a href="https://doi.org/10.18637/jss.v067.i04">https://doi.org/10.18637/jss.v067.i04</a></p>
</div>
<div id="ref-templ_simpop_2019">
<p>Templ, M., Kowarik, A., Meindl, B., Alfons, A., Ribatet, M., &amp; Gussenbauer, J. (2019). <em>simPop: Simulation of Synthetic Populations for Survey Data Considering Auxiliary Information</em>. Retrieved from <a href="https://CRAN.R-project.org/package=simPop">https://CRAN.R-project.org/package=simPop</a></p>
</div>
<div id="ref-templ_sdcmicro_2020">
<p>Templ, M., Meindl, B., &amp; Kowarik, A. (2020). <em>sdcMicro: Statistical Disclosure Control Methods for Anonymization of Data and Risk Estimation</em>. Retrieved from <a href="https://CRAN.R-project.org/package=sdcMicro">https://CRAN.R-project.org/package=sdcMicro</a></p>
</div>
<div id="ref-us_department_of_health__human_services_health_nodate">
<p>U.S. Department of Health &amp; Human Services. (n.d.). <em>Health Information Privacy</em>. Retrieved from <a href="https://www.hhs.gov/hipaa/index.html">https://www.hhs.gov/hipaa/index.html</a></p>
</div>
<div id="ref-varian_economic_2002">
<p>Varian, H. R. (2002). Economic Aspects of Personal Privacy. In W. H. Lehr &amp; L. M. Pupillo (Eds.), <em>Cyber Policy and Economics in an Internet Age</em> (pp. 127–137). <a href="https://doi.org/10.1007/978-1-4757-3575-8_9">https://doi.org/10.1007/978-1-4757-3575-8_9</a></p>
</div>
<div id="ref-woo_global_2009">
<p>Woo, M., Reiter, J. P., Oganian, A., &amp; Karr, A. F. (2009). Global Measures of Data Utility for Microdata Masked for Disclosure Limitation. <em>Privacy and Confidentiality</em>, <em>1</em>(1), 111–124. Retrieved from <a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=1006&amp;context=jpc">http://repository.cmu.edu/cgi/viewcontent.cgi?article=1006&amp;context=jpc</a></p>
</div>
<div id="ref-world_bank_dime_nodate">
<p>World Bank. (n.d.). <em>DIME Wiki: De-identification</em>. Retrieved from <a href="https://dimewiki.worldbank.org/wiki/De-identification">https://dimewiki.worldbank.org/wiki/De-identification</a></p>
</div>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Other terms sometimes used are “anonymization” or “de-identification,” but as this chapter will show, de-identification is a particular method of SDL, and anonymization is a goal, never fully achieved, rather than a method.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>The chapter on IAB provides a good illustration of how various SDL methods are combined with different access methods to provide multiple combinations of analytic validity and risk of disclosure.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Thus the occasional reference to methods as <em>deidentification</em> or <em>anonymization</em>, though these terms can sometimes be misleading in regard to what they can actually achieve.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>As of the writing of this chapter in August 2020, WP22 is being revised and updated, but has not yet been published.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>See guidance in <span class="citation">World Bank (<a href="#ref-world_bank_dime_nodate" role="doc-biblioref">n.d.</a>)</span> and <span class="citation">Kopper et al. (<a href="#ref-kopper_j-pal_2020" role="doc-biblioref">2020</a>)</span> .<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>One approach is to replace suppressed cells with imputed values, and then treat the data as multiply-imputed.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>See “<a href="https://stats.idre.ucla.edu/stata/seminars/mi_in_stata_pt1_new/">Multiple imputation in Stata</a>” or the <code>mice</code> package in R <span class="citation">(Buuren &amp; Groothuis-Oudshoorn, <a href="#ref-buuren_mice_2011" role="doc-biblioref">2011</a>)</span>.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
