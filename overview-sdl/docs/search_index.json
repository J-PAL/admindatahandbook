[
["index.html", "Overview of disclosure avoidance methods 0.1 About the Authors 0.2 References", " Overview of disclosure avoidance methods Ian M. Schmutte and Lars Vilhuber 2020-07-17 The overall purpose of this handbook is to provide guidance on how to enable broader access to data. As outlined in the introduction, one key concern is how to prevent unethical or illegal disclosure of data that should be kept confidential. The [chapter on physical protections] provides guidance on how to prevent disclosure - or release - of data or information by technical means. This chapter identifies the statistical means - through probabilistic or deterministic edits to data and outputs - by which data curators can limit the likelihood of a disclosure. These techniques are referred to as “statistical disclosure limitation” (SDL) in the literature, though one will sometimes find the use of “anonymization” or “de-identification.” We will define these terms, clarify what “limitation” means, why true anonymization is never possible, and why de-identification is very hard. We describe common traditional methods, deferring the most recent - and strict - method, differential privacy, to the more in-depth chapther on that topic. Because any SDL will modify data, in a way that distorts the true, measured value, the quality of any inference using the modified data will be (negatively) affected. We will describe a few ways used to measure the outcomes of these methods, i.e., how a particular method solves the tradeoff between disclosure limitation and degradation in quality. We note right away that there is no “correct” or “right” level of tradeoff - it is a choice made by the data custodians and policy makers in ways that must take into accounts the uses made of the data. We also discuss the impact various methods might have on data releases and econometric analyses of protected data. An often overlooked aspect of SDL methods is that they are not ignorable. However, an analogy is the care many econometric analyses take with addressing and correcting for measurement issues and biases. The distortion introduced through SDL methods is similar in impact, and often less in scale. This section draws heavily on Abowd &amp; Schmutte (2015). Finally, SDL methods must be implemented and deployed, and we provide pointers to existing “off-the-rack” tools in a variety of platforms (Stata, R, and Python). 0.0.1 Purpose of statistical disclosure limitation methods: Definitions and Context Discuss legal and ethical requirements. Define terms, with reference to external sources (de-identification, anonymization, pseudonymization, European, US, and Indian legislation). Link to “safe data” and “safe outputs”. Reference glossaries where available. (1-1.5 pages) The key concepts in disclosure avoidance are privacy and confidentiality. Privacy in the context of information can be viewed as the right to restrict (prevent) others’ gaining access to information a person holds, whether through query or through observation (see Hirshleifer (1980)). Confidentiality pertains to the information gained once privacy has been pierced (or breached), and applies to data that others hold about entities such as individuals or firms. Confidential should mean that the dissemination of data in a manner that would allow public identification of the respondent or would in any way be harmful to him is prohibited and that the data are immune from legal process. (Duncan, Jabine, &amp; Wolf (1993), p. 24.) In the United States, a majority of individuals are aware and possibly resigned that government and private companies collect data on them (about 62%, Auxier et al. (2019)), and seem to believe that there is little benefit to them of such collection: 81% think so when companies do the data collection, and 66% when the government does so (Auxier et al., 2019). Protection of the confidentiality of the underlying micro-data: - Avoiding identity disclosure: who (or what entity) is in the confidential micro-data - Avoiding attribute disclosure: value of a characteristic for that entity or individual - Avoiding inferential disclosure: improvement of the posterior odds of a particular event (identity or attribute) Within national statistical agencies, the primary approach to protecting respondent privacy has been statistical disclosure limitation or SDL. Fellegi (1972) initiated the statistical analysis of data confidentiality. Dalenius (1977) recognized that statistical agencies would need to do more than just protect against direct disclosures, and thus warned against what he called inferential disclosure. His idea was formalized by Duncan &amp; Lambert (1986), and provides the ultimate rationale for formal privacy in national statistics. Harris-Kojetin et al. (2005) provides the most comprehensive review of SDL methods currently in use across the U.S. statistical system. See also Dupriez &amp; Boyko (2010) for an overview from a multinational NSO perspective. Anderson &amp; Seltzer (2007) describes the history of threats to confidentiality in the U.S. statistical system prior to 1965. The interested reader is also pointed at a large and robust literature on (the value of) privacy in economics. That literature is generally focused on the value to individuals of being able to conceal private information, like a health condition, from a firm or prospective employer. The challenge to the firm is to design mechanisms, like pricing strategies, that encourage people to disclose private information. For an overview of ideas in this literature, we recommend reading Stigler (1980), Posner (1981), and Hirshleifer (1980). Varian (2002) and Acquisti, Taylor, &amp; Wagman (2016) both provide comprehensive surveys at different points in the development of this literature. 0.0.2 Methods Define the methods used for disclosure avoidance. (removal, coarsening of various kinds, swapping, data synthesis, formal privacy). Should reference WP 22, accessible guidelines by various entities including J-PAL, FCSM. Note: WP-22 is being revised, and will be released AFTER the handbook is released - so needs to have some forward looking elements. Reference glossaries where available. Harris-Kojetin et al. (2005) provides the most comprehensive review of SDL methods currently in use across the U.S. statistical system. See also Dupriez &amp; Boyko (2010) for an overview from a multinational NSO perspective. Garfinkel (2015) discusses techniques for de-identifying data and the many ways in which modern computing tools and a data-rich environment may render effective de-identification impossible. We briefly outline the most common methods here. (pull from Ian slides) Traditional methods - Suppression - Coarsening - Adding noise via swapping - Adding noise via sampling Newer methods - Explicit noise infusion - Synthetic data - Formal privacy (not handled here, refer to DP chapter) 0.0.2.1 Suppression Also mention rules that point to “suppress regression coefficients when derived from less than x% of sample, or the p-percent rule” as they are applied to “safe output” rules in RDC and MOU scenarios. This is by far the most common technique. - Model the sensitivity of a particular data item or observation (“disclosure risk”) - Do not allow the release of data items that have excessive disclosure risk (primary suppression) - Do not allow the release of other data from which the sensitive item can be calculated (complementary suppression) Explicitly mention rules that point to “suppress regression coefficients when derived from less than x% of sample, or the p-percent rule” as they are applied to “safe output” rules in RDC and MOU scenarios. Can provide examples from HRS or PSID rules (less than x people in a cell) 0.0.2.2 Coarsening (describe coarsening in output rules, such as rounding of regression coefficients to x significant digits) Coarsening is the creation of a smaller number of categories from the variable in order to increase the number of cases in each cell Computer scientists call this “generalizing” Geographic coarsening: block-block group-tract-minor civil division-county-state-region Top coding of income is a form of coarsening All continuous variables in a micro-data file can be considered coarsened to the level of precision (significant digits) released This method is often applied to model-based data releases by restricting the number of significant digits that can be released (bring example of top-coding from IAN slides) Coarsen: variables with heavy tails (earnings, payroll), residuals (truncate range, suppress labels of range) Smooth: density estimation and quantiles, use a kernel density estimator to produce quantiles 0.0.2.3 Swapping Estimate the disclosure risk of certain attributes or individuals If the risk is too great, attributes of one data record are (randomly) swapped with the same attributes of another record If geographic attributes are swapped this has the effect of placing the risky attributes in a different location from the truth Commonly used in household censuses and surveys Rarely used with establishment data 0.0.2.4 Sampling Sampling is the original SDL technique By only selecting certain entities from the population on which to collect additional data (data not on the frame), uncertainty about which entity was sampled provides some protection In modern, detailed surveys, sampling is of limited use for SDL 0.0.2.5 Noise infusion Adding noise to the published item or to the underlying micro data to disguise the true value Example: QWIs and work place data in OTM Original method developed by Evans, Zayatz, &amp; Slanta (1998) QWI method: Abowd et al. (2009) , Abowd et al. (2012) 0.0.2.6 Synthetic data Synthetic data are created by estimating the posterior predictive distribution (PPD) of the release data given the confidential data; then sampling release data from the PPD conditioning on the actual confidential values The PPD is a parameter-free forecasting model for new values of the complete data matrix that conditions on all values of the underlying confidential data Needs references to SSB, SynLBD. (2-3 pages) 0.0.3 Metrics How do you measure risk, and the reduction in risk achieved by applying above methods? Mention uniqueness criteria, k-anonymity, l-diversity, matching metrics, etc. Count of suppressed cells k-anonymity: (Sweeney, 2002) l-diversity: (Machanavajjhala, Kifer, Gehrke, &amp; Venkitasubramaniam, 2007) Also measure the value (2-3 pages) 0.0.4 Impact on analyses What is the impact on analyses when using protected data (reprise Abowd-Schmutte, BPEA), but also Chetty + Friedman (AEA P&amp;P, JPC) and Green-McKinney-Abowd-Vilhuber (increased variance) as well as the various papers on MOE/ACS. Consider Dwork on alignment of DP methods and safe inference. Reference standard methods that are related Moulton correction (data aggregated - for whatever reason) Using ACS MOE as an example of incorporating uncertainty (regardless of source) Parts of Abowd-Schmutte, BPEA Abowd &amp; Schmutte (2015) review the SDL methods currently in use and discuss their application to economic data. They argue that the analysis of SDL-laden data is inherently compromised because the details of the SDL protections cannot be disclosed. If they cannot be disclosed, their consequences for inference are unknowable, and, as they show, potentially large. Measuring both is difficult, except in very particular cases. In this section, we outline metrics used to assess both the protection provided through SDL, as well as the reduction in “data quality”. (expand using your slides) 0.0.4.1 Suppression 0.0.4.2 Coarsening Also reference the reduction in inference precision when regression coefficients are rounded (with probably little cost) 0.0.4.3 Adding noise via swapping 0.0.4.4 Adding noise via sampling 0.0.4.5 Noise infusion (LARS: can reference McKinney et al on QWI noise) 0.0.4.6 Synthetic data (LARS: can reference our work on SynLBD/SSB analysis) (1-2 pages) 0.0.5 Value of Privacy and Data Accuracy Any researcher or data provider needs to evaluate the benefits from reducing the protection afforded through SDL techniques and the gain from the additional accuracy. One key challenge for implementing privacy systems lies in choosing the amount, or type, of privacy to provide. Answering this question requires some way to understand the individual and social value of privacy. J. Abowd &amp; Schmutte (2019) discuss the question of optimal privacy protection (see also Hsu et al. (2014) in the specific context of differential privacy). Part of the social value of privacy arises from its relationship to scientific integrity. While the law of information recovery suggests that improved privacy must come at the cost of increased error in published statistics, these effects might be mitigated through two distinct channels. First, people are more truthful in surveys if they believe their data is not at risk, as Couper, Singer, Conrad, &amp; Groves (2008) illustrate. Second, work in computer science (Dwork et al., 2015, p. @dwork_fienberg_2018) and statistics (Cummings, Ligett, Nissim, Roth, &amp; Wu, 2016) suggests a somewhat surprising benefit of differential privacy: protection against overfitting. It is equally necessary to develop a more robust understanding of why data is valuable in the first place, the overall social cost of increasing error in public statistics. This seems to be an area in which very little comprehensive theoretical or empirical research has been done. We nevertheless recommend what seem to be good starting points. Spencer (1985), who developed a decision-theoretic framework for modeling optimal data quality. On the empirical side, a handful of interesting use cases suggest techniques for uncovering the value of data. For example, Card, Mas, Moretti, &amp; Saez (2012) and Perez-Truglia (2016) show how workers respond to pay transparency policies, which give them new information about co-worker salaries. Spencer &amp; Seeskin (2015) use a calibration exercise to study the costs, measured in misallocated congressional seats, of reduced accuracy in population census data. These examples might be of interest to various (government) data providers, as well as researchers interested in implementing surveys linked to administrative data. 0.0.6 Tools A sketch with lots of links to Stata and R packages that implement disclosure avoidance methods. Some simple methods to compute metrics as well (usually integrated). Summary of methods/packages in Stata Summary of methods/packages in R (1-2 pages) 0.1 About the Authors Ian M. Schmutte Lars Vilhuber 0.1.1 Acknowledgements This chapter draws on John M. Abowd et al. (2019) and the INFO7470 class at Cornell University Abowd &amp; Vilhuber (2016) . We gratefully acknowledge the support of Alfred P. Sloan Foundation Grant G-2015-13903 and NSF Grant SES-1131848. 0.1.2 Disclaimer The views expressed in this paper are those of the authors and not those of the U.S. Census Bureau or other sponsors. 0.2 References "]
]
