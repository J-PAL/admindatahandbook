# Introduction

> In order to provide guidance to current and future researchers, data providers, and administrators, we plan to curate and edit a Handbook on making administrative data more broadly available for research and evidence-based policy.

This handbook focuses broadly speaking on scenarios where confidential or proprietary data were made accessible to a broad group of researchers, be it by private, non-profit, or government organizations. This should include access by academic researchers who are not primarily associated with the institution that is creating, collecting, or making accessible the data in question. In most scenarios, there continue to be some access restrictions, i.e. the handbook does not primarily focus on situations where data have been successfully anonymized and posted for anybody to use, although some scenarios may have an "open data"  component. The handbook includes both scenarios where the data provider charges for access, whether for profit or for cost recovery, and where access is free. 
Our goal is to give an outlook on the possibilities for research and experiments using  data that is non-trivial to access, and the many opportunities for improving policy once data are made accessible.

## Structure

The handbook has two distinct parts. The first part of the handbook provides an overview of existing solutions to common challenges encountered in the process of making administrative data accessible. This includes chapters by dedicated experts on specific ethical, legal, and technical challenges. 

For the second part of this handbook, we present a set of case studies that display the diverse experiences of those who have undertaken such endeavors. The case studies are written by researchers, data providers, or administrators who have tackled a particular set of constraints, and implemented a particular solution. Our goal is  to show how data access solutions were selected and how they worked in practice, guiding readers through the myriad of decisions for and against certain options; practices that have been found to be useful in a given context, but also choices that in retrospect might have been made differently. 

To facilitate use, case study contributions build on a common, comprehensive [template] of topics around administrative data access for research. On the one hand, the case studies discuss challenges that need to be considered, on the other they point to opportunities for innovative research and experiments, and highlight useful techniques in data quality, processing, or linking, including with simultaneously collected survey data. Each case study speaks to at least one of the topics identified in the template in detail. Not all topics are relevant in all contexts, and readers will see this reflected in the handbook contributions. As a whole, the collection of case studies aims to speak to the full range of  considerations, from the fundamental - why even think about making the data available? - to the most mundane (but often pressing) - how much does that computer infrastructure cost? 

## A core concern: Privacy and Confidentiality

Privacy (the right to not provide information) and confidentiality (the right to not have the information collected disseminated to others) (SOURCE) are key ethical concerns, shared in this environment by researchers, policy makers, and data providers. One solution to protect the former is not to collect data in the first place. A solution to the latter is not to make data accessible to others once collected. Neither efficiently makes use of the available information, where efficiency is defined relative to some need or use case: a policy that could be better planned if better information on citizens were available to policy makers, or research into a virus or cancer that would be more effective if detailed information on symptoms, environment, and genes were available. The conflicting goals - collecting and making data available on one hand, and the need to protect the privacy and confidentiality of the people and firms - make the choice a hard one. @DinurNissim2003 highlighted the fundamental dilemma starkly: for any set of collected data items - a database - from which information is made accessible or published, there exists a point where no further information can be made available without ultimately releasing the entire database, and therefore potentially violating confidentiality promises. 

Addressing the balance between those two conflicting goals has been an active research field at least since census takers stopped posting roll calls (see @Anderson20xx for a history of privacy and confidentiality in the context of the United States statistical system). In this handbook, the various case studies illustrate how the various agencies have struck a balance that was deemed acceptable by the relevant stakeholders. Most often, that balance cannot be quantified. It is implicit in the various rules surrounding access and dissemination of the research and data products. 

In the [chapter on privacy and ethics](#privacy-ethics), Salil Vadhan and co-authors detail the most recent advances in statistical and computational techniques, in particular a technique called "differential privacy" (DP) first proposed by @DworkEtAl2006 . 

At the time this chapter was first written (`r config$first_publish_date`), a lively debate surrounding the largest implementation of DP was going on (see sidebar). Concerns are expressed that  DP would prevent informative analysis when using public-use data released under DP. There are several conditions for that argument to be made, though, and critically, most of those conditions do not apply to the particular audience for our handbook.

First, in the US-centric debate, *new* public-use data using DP  replaces *existing* public-use data (going forward) that did *not* use DP, and the resulting *new* public-use data is now (potentially) of lower utility when using the same techniques and use cases as before.  Second, much of the debate is about population-level datasets, not samples or extracts.

In the context of this handbook, the key goal is to unlock data that have not previously had public exposure, whether via public-use microdata, public statistics, or more restricted research use. There is no comparison to previous public-use data, and there are no previous use-cases. In that scenario, DP has the potential to allow for publication of data that wasn't previously possible, and allay fears about disclosure that may otherwise inhibit use of the data. DP may, in fact, allow for better-quality publication compared to traditional disclosure avoidance methods (rounding, suppression, grouping, etc.). There are a few positive examples, and Salil and his team have contributed to some of those positive examples. Some examples are @Onthemap [@Ashwin2008] and @PSEO [@Foote2019].

The second argument may or may not apply to our audiences. When samples are involved, correctly applied DP allows to take sampling variability into account, which is itself protective, and allows one to reduce the infused DP noise. This is also true in some cases where population level statistics are not fully observed (for instance, when 80% of your data are missing some attribute and you are imputing those attributes). There are a few papers that show that in those cases, both DP and DP-like noise infusion add very little additional noise to parts of that data that do not need protection (large cells), and only become salient in areas where protection is needed (small cells, for instance those that would traditionally have been suppressed) [@Friedman2019 , @Mckinney2017]

Finally, this is all about creating public use data - including dashboards and the like. When it comes to researchers sign NDAs and properly treat the sensitive data, DP is less salient. While there are some who call for *all* analysis to be conducted on DP data, that is a less accepted version of events (and often is convoluted with the type of "analysis" being conducted). When such researcher direct access occurs (what I call "research inside the secure box"), it is specifically designed (and always has been!) to do so in the absence of viable public-use alternatives. And when research results are removed from the secure box (by the researcher, or by data custodians), some sort of disclosure avoidance is always applied that has far less intrusive/destructive properties (rounding, limited precision). In fact, access via a "secure box" mechanism (see the [chapter on physical security](#physical-security) is meant as an explicit circumvention or replacement of data publication (DP or otherwise). It is not fully disconnected from it - there is still scope for unintentional but harmful disclosure when researchers publish research results. 

> Might also want to reference the effort to rewrite FCSM WP-22, with past and future disclosure avoidance techniques.

## What is different about this handbook

There are already many excellent resources on administrative data access: the various challenges of making data available securely  (see e.g. @Reuter2010-xr, @Harron2017-qp,  @ADRF_Network2018-wl,  @Future_of_Privacy_Forum2017-kj ); resources on data held by national statistical offices (NSO) and the initial creation of integrated data systems, including (in the U.S.) work by [Actionable Intelligence for Social Policy (AISP)](https://www.aisp.upenn.edu/); the general "Five Safes" framework [@DesaiFiveSafesdesigning2016]; and guides for the European context, which include case studies ( @OECDEXPERTGROUPINTERNATIONAL2014 , @Bujnowska2019-eq). 
In this handbook, we hope to provide an accessible overview and concrete examples of applications of the different available solutions in real-world contexts, embedded in successful cooperations of researchers and data providers which pushed the research frontier and contributed to innovative, evidence-based policy solutions.

The handbook is also meant to be a dynamic resource. In whatever medium you are reading this, the last update to this particular version was made on `r  config$last_update`. If you are reading a printed copy, then a more up-to-date version may exist at [`r config$base_url`](`r config$base_url`). We describe how to contribute in a later section.

